{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://github.com/hyeonsangjeon/Hyperparameters-Optimization/blob/master/pic/hyperparameteroptimization.png?raw=true\" alt=\"Hyperparameter Optimization\" width=\"400\"/></div>\n",
    "\n",
    "# Hyperparameter Optimization Tutorial\n",
    "\n",
    "## üìã Overview\n",
    "\n",
    "Hyperparameter tuning is **essential but time-consuming**. Simple models may take hours, while complex neural networks can require days or weeks.\n",
    "\n",
    "This tutorial provides hands-on practice with **5 major optimization techniques**, comparing their strengths, weaknesses, and practical applications.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Optimization Techniques Covered\n",
    "\n",
    "| Technique | Characteristics | Recommended For |\n",
    "|-----------|----------------|-----------------|\n",
    "| **1. Grid Search** | Exhaustive search of all combinations | Small parameter spaces |\n",
    "| **2. Random Search** | Random sampling | Quick prototyping |\n",
    "| **3. Optuna** | TPE + Early stopping | **Balanced choice (Recommended)** |\n",
    "| **4. Bayesian Optimization** | Probabilistic model-based | Maximum performance needs |\n",
    "| **5. TPE (Hyperopt)** | Bayesian optimization variant | High-dimensional problems |\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Learning Outcomes\n",
    "\n",
    "After completing this tutorial, you will:\n",
    "\n",
    "‚úÖ **Understand** how each technique works  \n",
    "‚úÖ **Choose** the right optimization method for different scenarios  \n",
    "‚úÖ **Reduce** modeling time effectively\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Important Note\n",
    "\n",
    "- **HyperBand**: Deprecated due to incompatibility with modern scikit-learn ‚Üí **Replaced with Optuna**\n",
    "- **Optuna**: Implements HyperBand's core concepts (early stopping, resource allocation) via TPE + Pruning\n",
    "- Provides more modern and powerful features with active maintenance\n",
    "\n",
    "---\n",
    "\n",
    "## üî¨ Experimental Setup\n",
    "\n",
    "| Component | Details |\n",
    "|-----------|---------|\n",
    "| **Dataset** | Sklearn Diabetes (442 samples) |\n",
    "| **Model** | LightGBM Regressor |\n",
    "| | üå≥ High-performance Gradient Boosting algorithm |\n",
    "| | Sequentially trains decision trees to correct previous errors |\n",
    "| | Fast training speed with high accuracy |\n",
    "| | Ideal for optimization learning with many tunable hyperparameters |\n",
    "| **Metric** | MSE (Mean Squared Error) |\n",
    "| **Validation** | 2-Fold Cross-Validation |\n",
    "| **Iterations** | 50 trials (same for all algorithms) |\n",
    "\n",
    "> **Note**: This is an educational demo. For production, use larger datasets and 5-fold or more cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pip install git+https://github.com/darenr/scikit-optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation Step\n",
    "- Import standard libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lightgbm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from lightgbm.sklearn import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings                                  # `do not disturb` mode\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Suppress LightGBM logs\n",
    "import os\n",
    "os.environ['LIGHTGBM_VERBOSE'] = '-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Dataset Preparation\n",
    "\n",
    "### üìä Sklearn Diabetes Dataset\n",
    "\n",
    "We use the **Sklearn Diabetes regression dataset** to compare hyperparameter optimization techniques.\n",
    "\n",
    "| Item | Details |\n",
    "|------|---------|\n",
    "| **Samples** | 442 patients |\n",
    "| **Features** | 10 (age, sex, BMI, blood pressure, etc.) |\n",
    "| **Target** | Disease progression after 1 year (continuous) |\n",
    "| **Problem Type** | Regression |\n",
    "\n",
    "### üí° Why This Dataset?\n",
    "\n",
    "‚úÖ **Fast experimentation**: Small size ideal for algorithm comparison  \n",
    "‚úÖ **Clear impact**: Regression problem shows hyperparameter effects intuitively  \n",
    "‚úÖ **Focus on optimization**: Concentrate on techniques rather than data complexity\n",
    "\n",
    "> **Note**: This tutorial focuses on **comparing optimization algorithms**. Concentrate on how each technique works rather than the dataset itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "n = diabetes.data.shape[0]\n",
    "\n",
    "data = diabetes.data\n",
    "targets = diabetes.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Data Splitting & Experiment Setup\n",
    "\n",
    "### üìä Data Split Strategy\n",
    "\n",
    "| Item | Setting | Description |\n",
    "|------|---------|-------------|\n",
    "| **Train/Test** | 80% / 20% | 353 training, 89 test samples |\n",
    "| **Cross-Validation** | 2-Fold KFold | Simplified for fast experimentation |\n",
    "| **Evaluation Metric** | MSE | Lower is better (prediction error) |\n",
    "| **Random Seed** | 42 | Ensures reproducible results |\n",
    "| **Iterations** | 50 trials | Same for all algorithms |\n",
    "\n",
    "### ‚ö†Ô∏è Experimental Constraints\n",
    "\n",
    "This tutorial is an **educational demo**. Differences from production:\n",
    "\n",
    "| Aspect | Tutorial | Production Recommendation |\n",
    "|--------|----------|---------------------------|\n",
    "| Data Size | 442 (small) | Thousands to tens of thousands |\n",
    "| Cross-Validation | 2-Fold | 5-Fold or more |\n",
    "| Iterations | 50 trials | 100-500 trials |\n",
    "| Stability Check | Single seed | Multiple seed testing |\n",
    "\n",
    "### üí° Learning Focus\n",
    "\n",
    "‚úÖ **Algorithm comparison** - Understand how each technique works  \n",
    "‚úÖ **Relative performance** - Which technique is more efficient  \n",
    "‚ùå **Absolute performance** - Not critical due to small dataset\n",
    "\n",
    "> **Next Step**: Now we'll split the data and prepare to run each optimization technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "\n",
    "# Experimental parameter settings\n",
    "random_state = 42  # Random seed for reproducible results\n",
    "n_iter = 50        # Number of iterations to apply to all optimization algorithms\n",
    "\n",
    "# Train/test data split (80% training, 20% testing)\n",
    "train_data, test_data, train_targets, test_targets = train_test_split(\n",
    "    data, \n",
    "    targets, \n",
    "    test_size=0.20, \n",
    "    shuffle=True,\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "# Cross-validation setup (2-fold KFold)\n",
    "num_folds = 2\n",
    "kf = KFold(\n",
    "    n_splits=num_folds, \n",
    "    shuffle=True, \n",
    "    random_state=random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data split results\n",
    "print('=' * 50)\n",
    "print('Data Split Results')\n",
    "print('=' * 50)\n",
    "print(f'Training Data   (train_data)    : {train_data.shape}')\n",
    "print(f'Test Data       (test_data)     : {test_data.shape}')\n",
    "print(f'Training Target (train_targets) : {train_targets.shape}')\n",
    "print(f'Test Target     (test_targets)  : {test_targets.shape}')\n",
    "print('=' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Creating Baseline Model\n",
    "\n",
    "### üéØ Model Selection: LGBMRegressor\n",
    "\n",
    "We'll solve this problem using `LGBMRegressor`. Gradient Boosting models have many tunable hyperparameters, making them ideal for demonstration.\n",
    "\n",
    "| Category | Description |\n",
    "|------|------|\n",
    "| **Algorithm** | LightGBM (Light Gradient Boosting Machine) |\n",
    "| **Model Type** | Regressor (regression prediction) |\n",
    "| **Feature** | Sequential learning of multiple trees |\n",
    "\n",
    "### üîÑ How It Works\n",
    "\n",
    "```\n",
    "Tree 1 ‚Üí Find errors ‚Üí Tree 2 compensates ‚Üí Tree 3 refines ‚Üí ... ‚Üí Final prediction\n",
    "```\n",
    "\n",
    "**Analogy**: Like solving exam problems\n",
    "- Student 1 solves and makes mistakes ‚Üí Student 2 corrects them\n",
    "- Student 2 makes mistakes ‚Üí Student 3 corrects them\n",
    "- Combine all students' answers ‚Üí Get final correct answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline model (using default parameters)\n",
    "model = LGBMRegressor(random_state=random_state, verbose=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìè Measuring Baseline Performance\n",
    "\n",
    "#### üí° Quick Summary\n",
    "**Before hyperparameter optimization**, we measure results from training **just once** with default settings.  \n",
    "‚Üí This score becomes our **comparison baseline**.\n",
    "\n",
    "#### üéØ Why We Need a Baseline\n",
    "\n",
    "```\n",
    "Baseline (default settings) vs Optimized results ‚Üí Compare improvement\n",
    "```\n",
    "\n",
    "| Category | Role |\n",
    "|------|------|\n",
    "| üìä **Comparison Standard** | MSE 3500 ‚Üí 2800 = Verify 20% improvement |\n",
    "| üí∞ **ROI Assessment** | Is the performance gain worth the optimization time invested? |\n",
    "| ‚è±Ô∏è **Practicality Evaluation** | 5 seconds vs 30 minutes‚Äîis the improvement worth it? |\n",
    "\n",
    "#### üî¨ Measurement Method\n",
    "\n",
    "| Item | Setting |\n",
    "|------|------|\n",
    "| **Evaluation Method** | 2-Fold Cross Validation |\n",
    "| **Evaluation Metric** | MSE (lower is better) |\n",
    "| **Parameters** | LightGBM default values |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Baseline model performance evaluation\n",
    "baseline_scores = cross_val_score(\n",
    "    model, \n",
    "    train_data, \n",
    "    train_targets, \n",
    "    cv=kf, \n",
    "    scoring=\"neg_mean_squared_error\", \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Calculate MSE (convert negative to positive)\n",
    "baseline_mse = -baseline_scores.mean()\n",
    "\n",
    "# Print results\n",
    "print('=' * 50)\n",
    "print('Baseline Model Performance')\n",
    "print('=' * 50)\n",
    "print(f'MSE (Mean): {baseline_mse:.2f}')\n",
    "print(f'MSE (Std Dev): {baseline_scores.std():.2f}')\n",
    "print(f'Individual Fold Results: {[-score for score in baseline_scores]}')\n",
    "print('=' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Setting Hyperparameter Search Space\n",
    "\n",
    "### üéØ Optimization Target: 3 Key Parameters\n",
    "\n",
    "| Parameter | Search Range | Role |\n",
    "|---------|---------|------|\n",
    "| **n_estimators** | 100 ~ 2000 | Number of trees (more accurate but slower) |\n",
    "| **max_depth** | 2 ~ 20 | Tree depth (deeper = learns complex patterns) |\n",
    "| **learning_rate** | 0.00001 ~ 1.0 | Learning rate (controls each tree's contribution) |\n",
    "\n",
    "### Why Only 3 Parameters?\n",
    "\n",
    "LightGBM has dozens of parameters, but in this tutorial:\n",
    "\n",
    "| Reason | Explanation |\n",
    "|------|------|\n",
    "| üéØ **Clear Comparison** | Focus on algorithm characteristics with core parameters |\n",
    "| ‚ö° **Fast Experiments** | Reduce execution time for better learning efficiency |\n",
    "| üìä **Easy Understanding** | Visualizations are easier to interpret |\n",
    "\n",
    "> **Important**: These 3 parameters alone have a major impact on model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Grid Search\n",
    "\n",
    "## Concept\n",
    "\n",
    "Grid Search is the most traditional optimization method that **exhaustively searches all hyperparameter combinations**.\n",
    "\n",
    "- **How It Works**: Try all combinations of parameter values specified by the user, and select the combination with the best cross-validation results\n",
    "- **Implementation**: Uses `sklearn.model_selection.GridSearchCV`\n",
    "\n",
    "## Pros and Cons\n",
    "\n",
    "### ‚úÖ Advantages\n",
    "- **Simple and Clear**: Easiest method to understand\n",
    "- **Complete Search**: Guaranteed to find optimal value within specified range\n",
    "- **Reproducible**: Always same results on the same grid\n",
    "\n",
    "### ‚ùå Disadvantages\n",
    "\n",
    "| Problem | Description | Example |\n",
    "|-----|------|-----|\n",
    "| **Slow Speed** | Time increases exponentially as all combinations are tried | Adding 1 parameter increases computation time 10x |\n",
    "| **Discrete Value Limitation** | May miss optimal values in between continuous values | If optimal is 550 but searching 100, 200, 300..., won't find it |\n",
    "| **Prior Knowledge Required** | Need to know appropriate search range in advance for efficiency | Wrong range wastes time |\n",
    "\n",
    "## Practical Example: Time Calculation\n",
    "\n",
    "**Full Grid (Unrealistic)**:\n",
    "- n_estimators: 20 values (100~2000)\n",
    "- max_depth: 19 values (2~20)\n",
    "- learning_rate: 5 values (0.0001~0.1)\n",
    "- **Total Combinations**: 20 √ó 19 √ó 5 = **1,900**\n",
    "- **Expected Time**: 15~30 minutes (excessive for 442 sample dataset)\n",
    "\n",
    "**Reduced Grid (This Tutorial)**:\n",
    "- n_estimators: 5 values (800~1200)\n",
    "- max_depth: 8 values (5~12)\n",
    "- learning_rate: 3 values (0.001~0.1)\n",
    "- **Total Combinations**: 5 √ó 8 √ó 3 = **120** ‚úÖ\n",
    "\n",
    "### üí° Practical Tips\n",
    "1. **Start with narrow range**: Gradual exploration from wide grid ‚Üí narrow grid\n",
    "2. **Important parameters first**: Allocate more values to parameters with greater impact\n",
    "3. **Consider time constraints**: Consider Random Search or Bayesian methods first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define Grid Search parameter grid (120 combinations)\n",
    "param_grid = {\n",
    "    'max_depth': np.linspace(5, 12, 8, dtype=int),        # 8 values\n",
    "    'n_estimators': np.linspace(800, 1200, 5, dtype=int), # 5 values\n",
    "    'learning_rate': np.logspace(-3, -1, 3),              # 3 values\n",
    "    'random_state': [random_state]\n",
    "}\n",
    "\n",
    "# Create GridSearchCV object\n",
    "gs = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    cv=kf,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Execute Grid Search\n",
    "gs.fit(train_data, train_targets)\n",
    "\n",
    "# Test set evaluation\n",
    "gs_test_score = mean_squared_error(test_targets, gs.predict(test_data))\n",
    "\n",
    "# Print results (same format as baseline)\n",
    "print('=' * 50)\n",
    "print('Grid Search Optimization Results')\n",
    "print('=' * 50)\n",
    "print(f'Optimal MSE (CV): {-gs.best_score_:.2f}')\n",
    "print(f'Test MSE: {gs_test_score:.2f}')\n",
    "print(f'Optimal Parameters:')\n",
    "for param, value in gs.best_params_.items():\n",
    "    if param != 'random_state':\n",
    "        print(f'  - {param}: {value}')\n",
    "print('=' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Grid Search Results Analysis\n",
    "\n",
    "### üîç Visualizing Parameter Search Process\n",
    "\n",
    "The graph below shows how each parameter changed while trying 120 combinations.\n",
    "\n",
    "### üí° Key Findings\n",
    "\n",
    "| Parameter | Impact | Characteristics |\n",
    "|---------|--------|------|\n",
    "| üìà **learning_rate** | üî• Very High | Lower values improve performance (most important) |\n",
    "| üìä **n_estimators** | ‚ö° Medium | Improves up to a certain level as tree count increases |\n",
    "| ‚ö†Ô∏è **max_depth** | üí§ Low | Relatively less impact ‚Üí 8 values is excessive |\n",
    "\n",
    "### ‚ùå Inefficiency of Grid Search\n",
    "\n",
    "```\n",
    "Problems:\n",
    "1. Searches all parameter combinations equally\n",
    "2. Uses 8 values even for low-impact max_depth ‚Üí 8x time increase\n",
    "3. Fixed grid approach ‚Üí Cannot consider parameter interactions\n",
    "```\n",
    "\n",
    "> **Conclusion**: Grid Search is thorough but **inefficient for the time invested**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Grid Search results to DataFrame\n",
    "gs_results_df = pd.DataFrame({\n",
    "    'score': -gs.cv_results_['mean_test_score'],\n",
    "    'learning_rate': gs.cv_results_['param_learning_rate'].data,\n",
    "    'max_depth': gs.cv_results_['param_max_depth'].data,\n",
    "    'n_estimators': gs.cv_results_['param_n_estimators'].data\n",
    "})\n",
    "\n",
    "# Visualize parameter exploration process\n",
    "gs_results_df.plot(subplots=True, figsize=(10, 10), title='Grid Search: Parameter Evolution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Random Search\n",
    "\n",
    "## Concept\n",
    "\n",
    "Random Search is a method that finds optimal values by **randomly sampling in the hyperparameter space**.\n",
    "\n",
    "- **How It Works**: Randomly extract parameter values from specified distributions and evaluate\n",
    "- **Implementation**: Uses `sklearn.model_selection.RandomizedSearchCV`\n",
    "- **Key Paper**: [Random Search for Hyper-Parameter Optimization (Bergstra & Bengio, 2012)](https://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/nslatysheva/data_science_blogging/master/expanding_ML_toolkit/expanding_toolkit.jpg\" style=\"height:500px;width:50%;\"/>\n",
    "\n",
    "## Pros and Cons\n",
    "\n",
    "### ‚úÖ Advantages\n",
    "\n",
    "| Advantage | Description | Compared to Grid Search |\n",
    "|-----|------|------------------|\n",
    "| **Efficient Exploration** | Tries more values for important parameters | Equal time on all parameters |\n",
    "| **Fast Speed** | Reaches good results with fewer iterations | Slow by trying all combinations |\n",
    "| **Continuous Value Support** | Freely explores real-valued parameters | Only searches discrete values |\n",
    "| **Scalability** | Time increases linearly even when adding parameters | Exponential increase when adding parameters |\n",
    "\n",
    "### ‚ùå Disadvantages\n",
    "\n",
    "1. **No Guarantee**: May not find optimal value within specified range\n",
    "2. **Independent Search**: Randomly selects each time without utilizing previous results\n",
    "3. **Depends on Luck**: Same settings may have different results depending on random_state\n",
    "\n",
    "## Practical Example\n",
    "\n",
    "In this tutorial:\n",
    "- **Wide Search Range**: learning_rate (10‚Åª‚Åµ ~ 1.0), n_estimators (100~2000), max_depth (2~20)\n",
    "- **Few Iterations**: Only 50 random samplings performed\n",
    "- **vs Grid Search**: 120 exhaustive search combinations vs 50 random samplings\n",
    "\n",
    "### üí° Practical Tips\n",
    "1. **Start with wide range**: First identify approximate area with Random Search\n",
    "2. **Sufficient iterations**: Minimum 50~100 recommended\n",
    "3. **Next step**: Based on Random Search results, narrow range for Grid Search or Bayesian methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing Random Search\n",
    "\n",
    "Uses `RandomizedSearchCV` to perform 50 random samplings in a wide parameter space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Define Random Search parameter distributions\n",
    "param_grid_rand = {\n",
    "    'learning_rate': np.logspace(-5, 0, 100),  # 10^-5 ~ 1.0 (continuous distribution)\n",
    "    'max_depth': randint(2, 20),               # 2 ~ 19 (uniform distribution)\n",
    "    'n_estimators': randint(100, 2000),        # 100 ~ 1999 (uniform distribution)\n",
    "    'random_state': [random_state]\n",
    "}\n",
    "\n",
    "# Create RandomizedSearchCV object\n",
    "rs = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_grid_rand,\n",
    "    n_iter=n_iter,                             # 50 random samplings\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    cv=kf,\n",
    "    verbose=False,\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "# Execute Random Search\n",
    "rs.fit(train_data, train_targets)\n",
    "\n",
    "# Test set evaluation\n",
    "rs_test_score = mean_squared_error(test_targets, rs.predict(test_data))\n",
    "\n",
    "# Print results (same format as baseline)\n",
    "print('=' * 50)\n",
    "print('Random Search Optimization Results')\n",
    "print('=' * 50)\n",
    "print(f'Optimal MSE (CV): {-rs.best_score_:.2f}')\n",
    "print(f'Test MSE: {rs_test_score:.2f}')\n",
    "print(f'Optimal Parameters:')\n",
    "for param, value in rs.best_params_.items():\n",
    "    if param != 'random_state':\n",
    "        print(f'  - {param}: {value}')\n",
    "print('=' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search Results Analysis\n",
    "\n",
    "### üìä Comparison with Grid Search\n",
    "\n",
    "**Performance:**\n",
    "- ‚úÖ Better results than Grid Search (with less time)\n",
    "- ‚úÖ 50 samplings vs 120 exhaustive search ‚Üí Superior efficiency per time\n",
    "\n",
    "### üé≤ Characteristics of Random Search\n",
    "\n",
    "**Advantages:**\n",
    "- Explores by **changing all parameters simultaneously** each time\n",
    "- No time wasted on less important parameters\n",
    "- Freely explores continuous value ranges\n",
    "\n",
    "**Limitations:**\n",
    "- Each sampling is **completely independent** ‚Üí Doesn't utilize previous results\n",
    "- Results may vary depending on luck\n",
    "\n",
    "### üí° Parameter Change Visualization\n",
    "\n",
    "The graph below shows Random Search **irregularly exploring the entire space**, unlike Grid Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Random Search results to DataFrame\n",
    "rs_results_df = pd.DataFrame({\n",
    "    'score': -rs.cv_results_['mean_test_score'],\n",
    "    'learning_rate': rs.cv_results_['param_learning_rate'].data,\n",
    "    'max_depth': rs.cv_results_['param_max_depth'].data,\n",
    "    'n_estimators': rs.cv_results_['param_n_estimators'].data\n",
    "})\n",
    "\n",
    "# Visualize parameter exploration process\n",
    "rs_results_df.plot(subplots=True, figsize=(10, 10), title='Random Search: Parameter Evolution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. HyperBand ‚ö†Ô∏è (Replaced with Optuna Due to Compatibility)\n",
    "\n",
    "### Research Paper [HyperBand](https://arxiv.org/pdf/1603.06560.pdf)\n",
    "\n",
    "## Concept\n",
    "\n",
    "HyperBand is an algorithm that maximizes optimization speed through **Early Stopping** and **Adaptive Resource Allocation**.\n",
    "\n",
    "**How It Works:**\n",
    "1. Randomly generate many hyperparameter combinations\n",
    "2. Start training each combination quickly with **few resources**\n",
    "3. **Terminate early** combinations with low performance\n",
    "4. Invest more resources only in promising combinations\n",
    "5. Finally, only a few best combinations train to completion\n",
    "\n",
    "**Analogy:** Similar to a hiring process that quickly screens dozens of candidates with short interviews, then only conducts 2nd and 3rd round interviews with promising few\n",
    "\n",
    "<img src=\"https://github.com/hyeonsangjeon/Hyperparameters-Optimization/blob/master/pic/Hyperband.png?raw=true\" />\n",
    "\n",
    "## ‚ö†Ô∏è Compatibility Issue\n",
    "\n",
    "**The scikit-hyperband library is no longer usable:**\n",
    "- Incompatible with scikit-learn 0.24+ (`iid` parameter removed)\n",
    "- Library maintenance discontinued\n",
    "- Cannot run in latest environments\n",
    "\n",
    "## Solution: Replace with Optuna\n",
    "\n",
    "HyperBand's core concepts can be implemented more modernly through **Optuna**.\n",
    "\n",
    "| Feature | HyperBand | Optuna |\n",
    "|-----|----------|--------|\n",
    "| **Core Technique** | Successive Halving | TPE (Bayesian) + Pruning (early stopping) |\n",
    "| **Strength** | Fast resource allocation optimization | Combines intelligent search + early stopping |\n",
    "| **Compatibility** | ‚ùå Not compatible with scikit-learn 0.24+ | ‚úÖ Fully compatible with latest libraries |\n",
    "| **Maintenance** | ‚ùå Discontinued | ‚úÖ Active development |\n",
    "\n",
    "### üí° Practical Selection Guide\n",
    "\n",
    "**When HyperBand concept is needed:**\n",
    "- ‚úÖ **Use Optuna** - Pruning feature implements HyperBand's early stopping concept\n",
    "- ‚úÖ Better performance: Synergy of Bayesian optimization + early stopping\n",
    "\n",
    "**Code below preserved for educational purposes only** (not executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperBand installation code (commented due to compatibility issues)\n",
    "# !git clone https://github.com/thuijskens/scikit-hyperband.git 2>/dev/null 1>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperBand file copy (commented due to compatibility issues)\n",
    "# !cp -r scikit-hyperband/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python setup.py install 2>/dev/null 1>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperBand execution code (commented due to compatibility issues)\n",
    "# Cannot run in scikit-learn 0.24+ due to removed iid parameter\n",
    "# Use Optuna section below instead\n",
    "\n",
    "\"\"\"\n",
    "%%time\n",
    "from hyperband import HyperbandSearchCV\n",
    "\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "param_hyper_band={'learning_rate': np.logspace(-5, 0, 100),\n",
    "                 'max_depth':  randint(2,20),\n",
    "                 'n_estimators': randint(100,2000),                  \n",
    "                 #'num_leaves' : randint(2,20),\n",
    "                 'random_state': [random_state]\n",
    "                 }\n",
    "\n",
    "\n",
    "hb = HyperbandSearchCV(model, param_hyper_band, max_iter = n_iter, scoring='neg_mean_squared_error', resource_param='n_estimators', random_state=random_state)\n",
    "\n",
    "\n",
    "#%time search.fit(new_training_data, y)\n",
    "hb.fit(train_data, train_targets)\n",
    "\n",
    "\n",
    "\n",
    "hb_test_score=mean_squared_error(test_targets, hb.predict(test_data))\n",
    "\n",
    "print('===========================')\n",
    "print(\"Best MSE = {:.3f} , when params {}\".format(-hb.best_score_, hb.best_params_))\n",
    "print('===========================')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-1 Optuna (Modern Alternative to HyperBand)\n",
    "\n",
    "## üîç Core Concept\n",
    "\n",
    "A modern hyperparameter optimization framework combining **Bayesian Optimization (TPE) + Early Stopping (Pruning)**\n",
    "\n",
    "```\n",
    "TPE: Learns from previous trial results to predict next parameters to try\n",
    "Pruning: Automatically terminates low-performing trials during training ‚Üí Saves time\n",
    "```\n",
    "\n",
    "**Analogy**: A system that analyzes past interview results to intelligently select next candidates, and immediately eliminates candidates certain to fail during the interview process\n",
    "\n",
    "## ‚öñÔ∏è HyperBand vs Optuna\n",
    "\n",
    "| Category | HyperBand | Optuna |\n",
    "|------|-----------|--------|\n",
    "| **Search Method** | Random + Early Stopping | Bayesian (intelligent) + Early Stopping |\n",
    "| **Learning Capability** | ‚ùå Doesn't use previous results | ‚úÖ Learns from previous results |\n",
    "| **Compatibility** | ‚ùå Only supports old versions | ‚úÖ Fully compatible with latest libraries |\n",
    "| **Maintenance** | ‚ùå Discontinued | ‚úÖ Active development (2024) |\n",
    "| **Visualization** | None | ‚úÖ Rich built-in tools |\n",
    "\n",
    "## üìä Pros & Cons\n",
    "\n",
    "### ‚úÖ Advantages\n",
    "\n",
    "- üß† **Intelligent Search**: Finds optimal values more efficiently than Random Search\n",
    "- ‚è±Ô∏è **Time Savings**: Pruning terminates unnecessary computations early\n",
    "- üîß **Flexibility**: Compatible with various ML frameworks (sklearn, PyTorch, etc.)\n",
    "\n",
    "### ‚ùå Disadvantages\n",
    "\n",
    "- ‚è∞ **Initial Overhead**: TPE model construction requires some time\n",
    "- üéõÔ∏è **Configuration Needed**: Additional setup for pruning strategy, sampler selection, etc.\n",
    "\n",
    "## üí° This Tutorial Setup\n",
    "\n",
    "```\n",
    "Number of trials: 50 (same as Random Search)\n",
    "Method: TPESampler (Bayesian optimization)\n",
    "Search space: \n",
    "  - learning_rate: 10‚Åª‚Åµ ~ 1.0\n",
    "  - n_estimators: 100 ~ 2000\n",
    "  - max_depth: 2 ~ 20\n",
    "```\n",
    "\n",
    "**Installation:**\n",
    "```bash\n",
    "pip install optuna\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and configure Optuna library\n",
    "# Installation: pip install optuna\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# Set log output level (show WARNING and above only)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Check version\n",
    "print(f\"Optuna version: {optuna.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 1. Define Optuna objective function\n",
    "def optuna_objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function to evaluate in Optuna trial\n",
    "    Returns: MSE (minimization target)\n",
    "    \"\"\"\n",
    "    # Suggest hyperparameters (TPE selects intelligently)\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1.0, log=True),  # Log scale\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 20),                          # Integer range\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 2000),                # Integer range\n",
    "        'random_state': random_state,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    # Create model and evaluate with cross-validation\n",
    "    model_optuna = LGBMRegressor(**params)\n",
    "    score = -cross_val_score(\n",
    "        model_optuna, \n",
    "        train_data, \n",
    "        train_targets, \n",
    "        cv=kf, \n",
    "        scoring=\"neg_mean_squared_error\", \n",
    "        n_jobs=-1\n",
    "    ).mean()\n",
    "    \n",
    "    return score\n",
    "\n",
    "# 2. Create Optuna Study\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',                      # Minimize MSE\n",
    "    sampler=TPESampler(seed=random_state)      # Use TPE algorithm\n",
    ")\n",
    "\n",
    "# 3. Execute optimization (50 trials)\n",
    "study.optimize(optuna_objective, n_trials=n_iter, show_progress_bar=True)\n",
    "\n",
    "# 4. Train final model with optimal parameters and test\n",
    "best_params_optuna = study.best_params.copy()\n",
    "best_params_optuna['random_state'] = random_state\n",
    "best_params_optuna['verbose'] = -1\n",
    "\n",
    "optuna_model = LGBMRegressor(**best_params_optuna)\n",
    "optuna_model.fit(train_data, train_targets)\n",
    "optuna_test_score = mean_squared_error(test_targets, optuna_model.predict(test_data))\n",
    "\n",
    "# 5. Print results (same format as baseline)\n",
    "print('=' * 50)\n",
    "print('Optuna Optimization Results')\n",
    "print('=' * 50)\n",
    "print(f'Optimal MSE (CV): {study.best_value:.2f}')\n",
    "print(f'Test MSE: {optuna_test_score:.2f}')\n",
    "print(f'Optimal Parameters:')\n",
    "for param, value in study.best_params.items():\n",
    "    print(f'  - {param}: {value}')\n",
    "print('=' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Optuna results to DataFrame\n",
    "optuna_results_df = pd.DataFrame({\n",
    "    'score': [trial.value for trial in study.trials],\n",
    "    'learning_rate': [trial.params['learning_rate'] for trial in study.trials],\n",
    "    'max_depth': [trial.params['max_depth'] for trial in study.trials],\n",
    "    'n_estimators': [trial.params['n_estimators'] for trial in study.trials]\n",
    "})\n",
    "\n",
    "# Visualize parameter exploration process (same format as other algorithms)\n",
    "optuna_results_df.plot(subplots=True, figsize=(10, 10), title='Optuna: Parameter Evolution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Bayesian Optimization\n",
    "\n",
    "## Concept\n",
    "\n",
    "Bayesian Optimization is an intelligent optimization method that **learns from previous results using probabilistic models**.\n",
    "\n",
    "**Core Idea:**\n",
    "- Unlike Random/Grid Search, **tracks and utilizes past evaluation results**\n",
    "- Estimates relationship between hyperparameters and performance scores as a **probabilistic model**: $P(\\text{Score} | \\text{Hyperparameters})$\n",
    "- **Strategically explores** the most promising areas\n",
    "\n",
    "![](https://github.com/hyeonsangjeon/Hyperparameters-Optimization/blob/master/pic/BayesianOpt.gif?raw=true)\n",
    "\n",
    "<img src=\"https://github.com/hyeonsangjeon/Hyperparameters-Optimization/blob/master/pic/bayesopt2.png?raw=true\" style=\"height:320px;\"  />\n",
    "\n",
    "## Core Components\n",
    "\n",
    "### 1. Surrogate Model\n",
    "\n",
    "**Definition:** A model that probabilistically estimates the objective function form based on points investigated so far $(x_1, f(x_1)), ..., (x_t, f(x_t))$\n",
    "\n",
    "- Denoted as $p(y | x)$ in papers\n",
    "- **Much cheaper computational cost** than actual objective function\n",
    "- Predicts performance in entire parameter space\n",
    "\n",
    "### 2. Acquisition Function\n",
    "\n",
    "**Definition:** A function that recommends the next input candidate $x_{t+1}$ most useful for finding the optimal input value $x^*$\n",
    "\n",
    "- Typically uses **Expected Improvement (EI)**\n",
    "- Balances Exploration and Exploitation\n",
    "\n",
    "## Algorithm Process\n",
    "\n",
    "1. Generate random initial point $x^*$\n",
    "2. Calculate $f(x^*)$\n",
    "3. Build conditional probability model $P(f | x)$ from past results (Surrogate Model)\n",
    "4. Select $x_i$ with highest probability of improving $f(x_i)$ according to $P(f | x)$ (Acquisition Function)\n",
    "5. Calculate actual value of $f(x_i)$\n",
    "6. Repeat steps 3~5 until maximum iterations reached\n",
    "\n",
    "## Pros and Cons\n",
    "\n",
    "### ‚úÖ Advantages\n",
    "\n",
    "| Advantage | Description |\n",
    "|-----|------|\n",
    "| **Intelligent Search** | Learns from previous results to focus on promising areas |\n",
    "| **Efficiency** | Can reach optimal with few iterations |\n",
    "| **Continuous Space Optimization** | Effective in continuous parameter spaces |\n",
    "\n",
    "### ‚ùå Disadvantages\n",
    "\n",
    "| Disadvantage | Description |\n",
    "|-----|------|\n",
    "| **Initial Cost** | Time required to build Surrogate model |\n",
    "| **High-Dimension Limitation** | Efficiency decreases as parameters increase (>20) |\n",
    "| **Implementation Complexity** | More complex to understand and implement than Random Search |\n",
    "\n",
    "## Practical Example\n",
    "\n",
    "In this tutorial:\n",
    "- Uses **scikit-optimize (skopt)** library's `BayesSearchCV`\n",
    "- **50 iterations** (same as Random Search)\n",
    "- **Wide Search Range**: learning_rate (10‚Åª‚Åµ ~ 1.0), n_estimators (100~2000), max_depth (2~20)\n",
    "\n",
    "### üí° Practical Tips\n",
    "\n",
    "1. **Initial Exploration**: First 10~20 iterations randomly explore to initialize model\n",
    "2. **Appropriate Iterations**: 50~100 iterations sufficient in most cases\n",
    "3. **High-Dimension Caution**: Consider TPE or Random Search if parameters exceed 10\n",
    "4. **Early Termination**: Can stop with callback function when target performance reached\n",
    "\n",
    "### üìö References\n",
    "\n",
    "- [Bayesian Optimization for Robots (Kaggle)](https://www.kaggle.com/artgor/bayesian-optimization-for-robots)\n",
    "- [scikit-optimize Documentation](https://scikit-optimize.github.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install scikit-optimize\n",
    "#https://towardsdatascience.com/hyperparameter-optimization-with-scikit-learn-scikit-opt-and-keras-f13367f3e796\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 1. Define search space\n",
    "search_space = {\n",
    "    'learning_rate': np.logspace(-5, 0, 100),  # 10^-5 ~ 1.0 (log scale)\n",
    "    \"max_depth\": Integer(2, 20),                # 2 ~ 20 (integer range)\n",
    "    'n_estimators': Integer(100, 2000),         # 100 ~ 2000 (integer range)\n",
    "    'random_state': [random_state]\n",
    "}\n",
    "\n",
    "# 2. Define callback function (early termination)\n",
    "def on_step(optim_result):\n",
    "    \"\"\"\n",
    "    Callback function called at each Bayesian Optimization iteration\n",
    "    Terminates optimization early when target performance reached\n",
    "    \"\"\"\n",
    "    score = optim_result.fun  # Minimum value so far (negative MSE)\n",
    "    print(f\"Best score: {score:.3f}\")\n",
    "    \n",
    "    # Terminate early if MSE better than 2800\n",
    "    if score < -2800:\n",
    "        print('Target performance reached! Stopping optimization.')\n",
    "        return True\n",
    "\n",
    "# 3. Create and execute BayesSearchCV object\n",
    "bayes_search = BayesSearchCV(\n",
    "    model, \n",
    "    search_space, \n",
    "    n_iter=n_iter,                          # 50 iterations\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    cv=kf,                                  # 2-fold CV (same as other algorithms)\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "# 4. Execute optimization (early termination possible with callback)\n",
    "bayes_search.fit(train_data, train_targets, callback=on_step)\n",
    "\n",
    "# 5. Test set evaluation\n",
    "bayes_test_score = mean_squared_error(test_targets, bayes_search.predict(test_data))\n",
    "\n",
    "# 6. Print results (same format as baseline)\n",
    "print('=' * 50)\n",
    "print('Bayesian Optimization Results')\n",
    "print('=' * 50)\n",
    "print(f'Optimal MSE (CV): {-bayes_search.best_score_:.2f}')\n",
    "print(f'Test MSE: {bayes_test_score:.2f}')\n",
    "print(f'Optimal Parameters:')\n",
    "for param, value in bayes_search.best_params_.items():\n",
    "    if param != 'random_state':\n",
    "        print(f'  - {param}: {value}')\n",
    "print('=' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Bayesian Optimization results to DataFrame\n",
    "bayes_results_df = pd.DataFrame({\n",
    "    'score': -bayes_search.cv_results_['mean_test_score'],\n",
    "    'learning_rate': bayes_search.cv_results_['param_learning_rate'].data,\n",
    "    'max_depth': bayes_search.cv_results_['param_max_depth'].data,\n",
    "    'n_estimators': bayes_search.cv_results_['param_n_estimators'].data\n",
    "})\n",
    "\n",
    "# Visualize parameter exploration process (same format as other algorithms)\n",
    "bayes_results_df.plot(subplots=True, figsize=(10, 10), title='Bayesian Optimization: Parameter Evolution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Hyperopt (TPE Implementation)\n",
    "\n",
    "## Overview\n",
    "\n",
    "**Hyperopt** is a Python library that implements the TPE (Tree-structured Parzen Estimator) algorithm.\n",
    "\n",
    "- **GitHub**: [hyperopt/hyperopt](https://github.com/hyperopt/hyperopt)\n",
    "- **Feature**: Automatic hyperparameter search based on Bayesian optimization\n",
    "- **Advantage**: Works independently of sklearn, allows flexible search space definition\n",
    "\n",
    "## Main Components\n",
    "\n",
    "| Function/Class | Role | Description |\n",
    "|-----------|-----|------|\n",
    "| **fmin** | Main optimization function | Searches optimal parameters that minimize objective function |\n",
    "| **tpe.suggest** | TPE algorithm | Bayesian optimization strategy (recommended) |\n",
    "| **hp** | Search space definition | Specifies parameter distributions (uniform, loguniform, etc.) |\n",
    "| **Trials** | Execution log | Stores results of all trials |\n",
    "\n",
    "**Installation:**\n",
    "```bash\n",
    "pip install hyperopt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Hyperopt Library\n",
    "\n",
    "Import necessary functions:\n",
    "- **fmin**: Execute hyperparameter optimization\n",
    "- **tpe**: TPE algorithm (Bayesian optimization)\n",
    "- **hp**: Define parameter search space (quniform, loguniform, etc.)\n",
    "- **Trials**: Log and track execution results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, anneal, Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Objective Function\n",
    "\n",
    "Unlike sklearn, Hyperopt requires **direct definition of objective function**. The function below takes parameters, trains model, and returns MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gb_mse_cv(params, random_state=random_state, cv=kf, X=train_data, y=train_targets):\n",
    "    \"\"\"\n",
    "    Hyperopt objective function: Trains LGBMRegressor with given parameters and returns MSE\n",
    "    \n",
    "    Args:\n",
    "        params (dict): Hyperparameters to optimize (n_estimators, max_depth, learning_rate)\n",
    "        \n",
    "    Returns:\n",
    "        float: Cross-validation MSE (minimization target)\n",
    "    \"\"\"\n",
    "    # 1. Convert parameters to integers and organize\n",
    "    model_params = {\n",
    "        'n_estimators': int(params['n_estimators']), \n",
    "        'max_depth': int(params['max_depth']), \n",
    "        'learning_rate': params['learning_rate'],\n",
    "        'random_state': random_state,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    # 2. Create model\n",
    "    model = LGBMRegressor(**model_params)\n",
    "    \n",
    "    # 3. Perform cross-validation and calculate MSE\n",
    "    score = -cross_val_score(\n",
    "        model, X, y, \n",
    "        cv=cv, \n",
    "        scoring=\"neg_mean_squared_error\", \n",
    "        n_jobs=-1\n",
    "    ).mean()\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-structured Parzen Estimator (TPE)\n",
    "\n",
    "### Research Paper\n",
    "[Algorithms for Hyper-Parameter Optimization (Bergstra et al., NIPS 2011)](https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf)\n",
    "\n",
    "<img src=\"https://github.com/hyeonsangjeon/Hyperparameters-Optimization/blob/master/pic/TPE.gif?raw=true\" />\n",
    "\n",
    "## Concept\n",
    "\n",
    "TPE is **Hyperopt's core algorithm**, a method that implements Bayesian optimization.\n",
    "\n",
    "**Core Idea:**\n",
    "- Learns from previous trial results to **intelligently predict parameters to try next**\n",
    "- Probabilistically distinguishes between parameter regions with good and bad performance\n",
    "- Focuses on promising regions while also exploring new regions (Exploration & Exploitation)\n",
    "\n",
    "## Algorithm Process\n",
    "\n",
    "1. **Initialize**: Generate random initial point $x^*$\n",
    "2. **Evaluate**: Calculate $F(x^*)$ (perform cross-validation)\n",
    "3. **Build Model**: Create conditional probability model $P(F | x)$ from past trial history\n",
    "4. **Select Next Candidate**: Based on $P(F | x)$, select $x_i$ with highest probability of improving $F(x_i)$\n",
    "5. **Actual Evaluation**: Calculate actual value of $F(x_i)$\n",
    "6. **Repeat**: Repeat steps 3~5 until maximum iterations reached\n",
    "\n",
    "## Comparison with Bayesian Optimization\n",
    "\n",
    "| Feature | Bayesian Optimization (skopt) | TPE (Hyperopt) |\n",
    "|-----|------------------------------|----------------|\n",
    "| **Surrogate Model** | Gaussian Process (GP) | Tree-structured Parzen Estimator |\n",
    "| **Computational Complexity** | O(n¬≥) - slow in high dimensions | O(n) - fast even in high dimensions |\n",
    "| **Parameter Independence** | Considers correlations | Assumes conditional independence |\n",
    "| **Application Scenario** | Parameters < 10 | Parameters > 10 |\n",
    "\n",
    "### üí° Practical Tips\n",
    "\n",
    "1. **High-Dimension Problems**: TPE more efficient than Bayesian Optimization when parameters exceed 10\n",
    "2. **Fast Feedback**: TPE builds models quickly, advantageous when preferring fast iteration\n",
    "3. **Flexible Search Space**: Can define conditional parameters (e.g., different parameters depending on number of layers)\n",
    "\n",
    "### üìö References\n",
    "\n",
    "- [Detailed TPE Algorithm Explanation (Towards Data Science)](https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f)\n",
    "- [Hyperopt Official Documentation](https://github.com/hyperopt/hyperopt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing TPE\n",
    "\n",
    "Performs TPE optimization using Hyperopt's `fmin` function. Compares under same conditions as Random Search with 50 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 1. Define search space\n",
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 100, 2000, 1),  # 100 ~ 2000 (integer)\n",
    "    'max_depth': hp.quniform('max_depth', 2, 20, 1),            # 2 ~ 20 (integer)\n",
    "    'learning_rate': hp.loguniform('learning_rate', -5, 0)      # 10^-5 ~ 1.0 (log scale)\n",
    "}\n",
    "\n",
    "# 2. Create Trials object (for logging)\n",
    "trials = Trials()\n",
    "\n",
    "# 3. Set random seed (reproducibility)\n",
    "np.random.seed(random_state)\n",
    "\n",
    "# 4. Execute TPE optimization\n",
    "best = fmin(\n",
    "    fn=gb_mse_cv,           # Objective function to minimize\n",
    "    space=space,            # Search space\n",
    "    algo=tpe.suggest,       # TPE algorithm\n",
    "    max_evals=n_iter,       # Maximum iterations (50)\n",
    "    trials=trials           # Result logging\n",
    ")\n",
    "\n",
    "# 5. Train final model with optimal parameters\n",
    "best_params_tpe = {\n",
    "    'n_estimators': int(best['n_estimators']),\n",
    "    'max_depth': int(best['max_depth']),\n",
    "    'learning_rate': best['learning_rate'],\n",
    "    'random_state': random_state,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "tpe_model = LGBMRegressor(**best_params_tpe)\n",
    "tpe_model.fit(train_data, train_targets)\n",
    "\n",
    "# 6. Test set evaluation\n",
    "tpe_test_score = mean_squared_error(test_targets, tpe_model.predict(test_data))\n",
    "\n",
    "# 7. Print results (same format as baseline)\n",
    "print('=' * 50)\n",
    "print('TPE (Hyperopt) Optimization Results')\n",
    "print('=' * 50)\n",
    "print(f'Optimal MSE (CV): {gb_mse_cv(best):.2f}')\n",
    "print(f'Test MSE: {tpe_test_score:.2f}')\n",
    "print(f'Optimal Parameters:')\n",
    "for param, value in best.items():\n",
    "    print(f'  - {param}: {value:.6f}' if isinstance(value, float) else f'  - {param}: {int(value)}')\n",
    "print('=' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPE Results Analysis\n",
    "\n",
    "TPE tends to find **better hyperparameter combinations with fewer iterations** than Random Search.\n",
    "\n",
    "- **Bayesian Approach**: Learns from results of previous trials to smartly select parameters to try next.\n",
    "- **Efficient Exploration**: Even with same 50 iterations, more likely to converge to higher quality solutions than Random Search.\n",
    "- **Balanced Performance**: Computation time increases slightly, but performance improvement is significant enough to be frequently used in practice.\n",
    "\n",
    "The graph below shows TPE's convergence process where **score gradually decreases (improves)** as iterations progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert TPE (Hyperopt) search results to DataFrame\n",
    "tpe_results = np.array([\n",
    "    [x['result']['loss'],\n",
    "     x['misc']['vals']['learning_rate'][0],\n",
    "     x['misc']['vals']['max_depth'][0],\n",
    "     x['misc']['vals']['n_estimators'][0]]\n",
    "    for x in trials.trials\n",
    "])\n",
    "\n",
    "tpe_results_df = pd.DataFrame(\n",
    "    tpe_results,\n",
    "    columns=['score', 'learning_rate', 'max_depth', 'n_estimators']\n",
    ")\n",
    "\n",
    "# Visualize parameter exploration process (same format as other algorithms)\n",
    "tpe_results_df.plot(subplots=True, figsize=(10, 10), title='TPE (Hyperopt): Parameter Evolution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compares **optimal performance (best cumulative score) changes over iterations** for all approaches in one graph.\n",
    "\n",
    "- The y-axis represents the **trend of best (minimum) MSE value** found by each algorithm so far.\n",
    "- The faster and lower the curve descends, **the better hyperparameters were found with fewer attempts**.\n",
    "- The **Baseline** curve represents the fixed MSE without any tuning, and how much other curves fall below this line is **the gain (performance improvement) from tuning**.\n",
    "- Through this graph, you can intuitively compare **how much each method improved vs Baseline**, and **convergence speed and final performance** among Grid/Random/Optuna/Bayesian/TPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare best cumulative score changes over iterations for all algorithms\n",
    "scores_df = pd.DataFrame(index=range(n_iter))\n",
    "scores_df['Baseline'] = baseline_mse\n",
    "scores_df['Grid Search'] = gs_results_df['score'].cummin()\n",
    "scores_df['Random Search'] = rs_results_df['score'].cummin()\n",
    "# scores_df['Hyperband'] = hb_results_df['score'].cummin()  # Activate if needed\n",
    "scores_df['Optuna'] = optuna_results_df['score'].cummin()\n",
    "scores_df['Bayesian Optimization'] = bayes_results_df['score'].cummin()\n",
    "scores_df['TPE'] = tpe_results_df['score'].cummin()\n",
    "\n",
    "# Visualize convergence curves\n",
    "ax = scores_df.plot(figsize=(10, 6))\n",
    "ax.set_xlabel('Number of iterations')\n",
    "ax.set_ylabel('Best cumulative score (MSE)')\n",
    "ax.set_title('Hyperparameter Optimization: Best Score over Iterations')\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Analysis\n",
    "\n",
    "#### üìä Algorithm Performance Comparison\n",
    "\n",
    "**Improvement vs Baseline:**\n",
    "- All optimization algorithms achieve **significantly lower MSE** than Baseline ‚Üí Hyperparameter tuning effect is clear\n",
    "\n",
    "**Algorithm Characteristics:**\n",
    "\n",
    "| Algorithm | Convergence Speed | Final Performance | Computational Cost | Recommended Situation |\n",
    "|---------|----------|----------|----------|----------|\n",
    "| **Grid Search** | Slow | Good | High | When parameter space is small and precise search needed |\n",
    "| **Random Search** | Fast | Good | Low | Fast prototyping, initial experiments |\n",
    "| **Optuna** | Very Fast | Very Good | Medium | **Balanced choice** - recommended in most cases |\n",
    "| **Bayesian Opt** | Medium | Very Good | High | Parameters < 10, highest performance needed |\n",
    "| **TPE** | Fast | Very Good | Medium | Parameters > 10, high-dimensional problems |\n",
    "\n",
    "#### üí° Practical Application Guide\n",
    "\n",
    "**Situation-Specific Recommendations:**\n",
    "1. **‚ö° Fast Prototyping** (Time Constraints)\n",
    "   - Priority 1: **Random Search** - Simple and fast\n",
    "   - Priority 2: **Optuna** - Better results with slightly more time\n",
    "\n",
    "2. **üéØ Highest Performance Needed** (Time Available)\n",
    "   - Parameters < 10: **Bayesian Optimization**\n",
    "   - Parameters ‚â• 10: **TPE (Hyperopt)** or **Optuna**\n",
    "\n",
    "3. **‚öñÔ∏è Balanced Choice** (Practical Recommendation)\n",
    "   - **Optuna (TPE + Pruning)** - Optimal balance of speed and performance\n",
    "\n",
    "4. **üöÄ Limited Time/Resources**\n",
    "   - **Utilize Optuna's Pruning feature** - Early termination of unnecessary trials\n",
    "\n",
    "#### ‚ö†Ô∏è Important Notes\n",
    "\n",
    "**Cautions When Interpreting Graphs:**\n",
    "- This experiment was conducted with **small dataset (442 samples), 2-fold CV, 50 iterations**\n",
    "- Results may vary greatly depending on `random_state` value\n",
    "- More folds (5-fold or more) and more iterations recommended in actual environments\n",
    "\n",
    "**Toy Dataset Characteristics:**\n",
    "- **Overfitting easily occurs** on small datasets (442 samples)\n",
    "- When CV MSE is high but test MSE is low ‚Üí May be due to test set being coincidentally easy or data being too small\n",
    "- **Validation on larger datasets needed** in actual projects\n",
    "\n",
    "**Practical Evaluation Criteria:**\n",
    "- This tutorial: Comparison based on iteration count (educational purpose)\n",
    "- Actual environment: Evaluation based on **actual time spent** is more accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data Performance Comparison\n",
    "\n",
    "Outputs **test set MSE** for all algorithms to verify practical performance.\n",
    "\n",
    "### üìã Comparison Items\n",
    "- **Cross-Validation MSE**: Optimal performance obtained during training for each algorithm (refer to graph above)\n",
    "- **Test MSE**: Actual prediction performance on unseen test data (output below)\n",
    "\n",
    "### üí° Interpretation Guide\n",
    "- **CV MSE < Test MSE**: Normal case - better performance on training data\n",
    "- **CV MSE ‚âà Test MSE**: Ideal - excellent generalization performance\n",
    "- **CV MSE > Test MSE**: Toy dataset characteristic - possibility that test set was coincidentally easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set performance comparison (including baseline)\n",
    "print('=' * 50)\n",
    "print('Test Set MSE Comparison')\n",
    "print('=' * 50)\n",
    "print(f\"Baseline (no tuning)    : {baseline_mse:.3f}\")\n",
    "print('-' * 50)\n",
    "print(f\"Grid Search             : {gs_test_score:.3f}\")\n",
    "print(f\"Random Search           : {rs_test_score:.3f}\")\n",
    "print(f\"Optuna                  : {optuna_test_score:.3f}\")\n",
    "print(f\"Bayesian Optimization   : {bayes_test_score:.3f}\")\n",
    "print(f\"TPE (Hyperopt)          : {tpe_test_score:.3f}\")\n",
    "print('=' * 50)\n",
    "\n",
    "# Calculate improvement rate vs baseline\n",
    "print('\\nImprovement Rate vs Baseline:')\n",
    "print('=' * 50)\n",
    "for name, score in [\n",
    "    ('Grid Search', gs_test_score),\n",
    "    ('Random Search', rs_test_score),\n",
    "    ('Optuna', optuna_test_score),\n",
    "    ('Bayesian Optimization', bayes_test_score),\n",
    "    ('TPE (Hyperopt)', tpe_test_score)\n",
    "]:\n",
    "    improvement = ((baseline_mse - score) / baseline_mse) * 100\n",
    "    print(f\"{name:25s}: {improvement:+.2f}%\")\n",
    "print('=' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Conclusion\n",
    "\n",
    "### üèÜ Algorithm Characteristics Summary\n",
    "\n",
    "| Algorithm | Recommended Situation | Advantages | Disadvantages |\n",
    "|---------|----------|-----|------|\n",
    "| **Optuna** | **Most Cases** | Optimal balance of speed and performance | Initial setup required |\n",
    "| **Random Search** | Quick experiments | Simple and fast | No guarantee of best performance |\n",
    "| **TPE (Hyperopt)** | High-dimensional parameters | Effective for many parameters | Learning curve exists |\n",
    "| **Bayesian Opt** | Highest performance needed | High final performance | Long computation time |\n",
    "| **Grid Search** | Small search space | Guaranteed complete search | High time consumption |\n",
    "\n",
    "### üí° Practical Selection Guide\n",
    "\n",
    "**When Time Constrained:**\n",
    "- Start with Random Search ‚Üí Find good range ‚Üí Precise search with Optuna\n",
    "\n",
    "**When Highest Performance Needed:**\n",
    "- Parameters < 10: Bayesian Optimization\n",
    "- Parameters ‚â• 10: TPE or Optuna\n",
    "\n",
    "**In Most Cases:**\n",
    "- Use **Optuna** - Modern, actively maintained, and powerful\n",
    "\n",
    "### ‚ö†Ô∏è Important Notice\n",
    "\n",
    "This experiment was conducted with **toy dataset for educational purposes** (442 samples, 2-fold CV).\n",
    "\n",
    "**In Actual Projects:**\n",
    "- More data (minimum thousands of samples)\n",
    "- More folds (5-fold or more)\n",
    "- Validation with multiple random_state\n",
    "- Evaluation based on actual time spent\n",
    "\n",
    "Recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Summary\n",
    "\n",
    "### üéØ 5 Optimization Algorithms Compared\n",
    "\n",
    "| No. | Algorithm | Core Feature | Practical Rating |\n",
    "|-----|---------|----------|-----------|\n",
    "| 1 | **Grid Search** | Exhaustive search of all combinations | ‚≠ê‚≠ê (Small space only) |\n",
    "| 2 | **Random Search** | Random sampling | ‚≠ê‚≠ê‚≠ê‚≠ê (Quick start) |\n",
    "| 3 | **Optuna** | TPE + Pruning combined | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Best balance) |\n",
    "| 4 | **Bayesian Opt** | Gaussian Process based | ‚≠ê‚≠ê‚≠ê‚≠ê (High performance) |\n",
    "| 5 | **TPE (Hyperopt)** | Bayesian optimization variant | ‚≠ê‚≠ê‚≠ê‚≠ê (High dimensions) |\n",
    "\n",
    "### üí° Situation-Based Selection Guide\n",
    "\n",
    "**Project Start Phase:**\n",
    "1. Identify good parameter range with Random Search (1 hour)\n",
    "2. Narrow range for precise search with Optuna (2-3 hours)\n",
    "3. Final fine-tuning if needed (additional 1 hour)\n",
    "\n",
    "**Time-Based Recommendations:**\n",
    "- ‚ö° **Within 30 minutes**: Random Search (50 iterations)\n",
    "- üéØ **1-2 hours**: Optuna (100 iterations + Pruning)\n",
    "- üèÜ **3+ hours**: Bayesian Opt or TPE (200+ iterations)\n",
    "\n",
    "**By Parameter Count:**\n",
    "- Parameters < 5: Grid Search also possible\n",
    "- Parameters 5-10: Bayesian Optimization or Optuna\n",
    "- Parameters > 10: **TPE** or **Optuna** (essential)\n",
    "\n",
    "### üöÄ Key Lessons\n",
    "\n",
    "**There is no \"perfect\" algorithm.**\n",
    "- Random Search can be better than Bayesian depending on situation\n",
    "- What matters is **performance improvement per time spent**\n",
    "- In most cases **Optuna is most practical**\n",
    "\n",
    "**Practical Tips:**\n",
    "1. Measure baseline first (confirm improvement effect)\n",
    "2. Start with few iterations ‚Üí Gradually expand\n",
    "3. Monitor exploration process with visualization\n",
    "4. Evaluate based on actual time spent\n",
    "\n",
    "---\n",
    "\n",
    "Hope this tutorial helps **drastically reduce hyperparameter optimization time** in your ML/DL projects! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
