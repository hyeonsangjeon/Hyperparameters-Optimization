# ğŸš€ Hyperparameter Optimization Tutorial

<div align="center">
  <img src="pic/hyperparameteroptimization.png" alt="Hyperparameter Optimization" width="300"/>
</div>

> **Practical comparison of 5 hyperparameter optimization algorithms for machine learning**

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-orange.svg)](https://jupyter.org/)
[![LightGBM](https://img.shields.io/badge/LightGBM-Latest-brightgreen)](https://github.com/microsoft/LightGBM)

[ğŸ‡°ğŸ‡· í•œêµ­ì–´](HyperParameterInspect.ipynb) | [ğŸ‡¬ğŸ‡§ English](HyperParameterInspect_EN.ipynb) | [ğŸ¯ Quick Start](#-quick-start) | [ğŸ“Š Results](#-key-results)

---

## âš¡ Key Results

**Typical Performance Pattern (Diabetes Dataset: 442 samples, 10 features, 50 iterations)**

| Method | Typical Improvement | Speed | Best For |
|--------|-------------------|-------|----------|
| **TPE (Hyperopt)** | ~27% â­â­ | **Fastest** âš¡ | **Best overall performance** |
| **Random Search** | ~26% â­ | Fast | Quick prototyping, reliable |
| **Optuna (TPE+Pruning)** | ~26% â­ | Fast | Production systems |
| **Bayesian Optimization** | ~26% â­ | Moderate | Critical performance needs |
| **Grid Search** | ~22% | Slow | Small search spaces |
| *Baseline (default)* | *0%* | *-* | *Reference point* |

> ğŸ’¡ **Important Note**: Actual results vary based on random_state, data split, and environment. All methods typically improve baseline by 20-27%. Run the notebook to see results on your machine.

> âš¡ **Key Insight**: TPE (Hyperopt) achieved the highest improvement (+27.12%), closely followed by Random Search (+26.33%) and Optuna (+26.02%). Modern Bayesian methods consistently outperform Grid Search with better efficiency.

---

## ğŸ“ What You'll Learn

### ğŸ“š Five Optimization Algorithms

1. **Grid Search** - Exhaustive search through all parameter combinations
2. **Random Search** - Random sampling from parameter distributions  
3. **Optuna** - Modern TPE with pruning (replaces deprecated HyperBand)
4. **Bayesian Optimization** - Probabilistic model-based optimization
5. **TPE (Hyperopt)** - Tree-structured Parzen Estimator

### ğŸ¯ Learning Outcomes

- Understand strengths and weaknesses of each algorithm
- Know which method to choose for different scenarios
- Implement optimization in real projects with working code
- Compare results with statistical rigor
- Reduce hyperparameter tuning time significantly

---

## ğŸš€ Quick Start

### Installation

```bash
git clone https://github.com/hyeonsangjeon/Hyperparameters-Optimization.git
cd Hyperparameters-Optimization
pip install -r requirements.txt
```

### Run Tutorial

**Interactive Notebook** (Recommended)
```bash
jupyter notebook HyperParameterInspect.ipynb        # Korean
jupyter notebook HyperParameterInspect_EN.ipynb     # English
```

**Automated Benchmark**
```bash
python benchmark_hpo_algorithms.py
```

---

## ğŸ“Š Algorithm Comparison

### Selection Guide

| Your Scenario | Recommended | Why |
|---------------|-------------|-----|
| **Quick prototyping** | Random Search | Fast setup, decent results |
| **Production deployment** | Optuna | Modern, pruning, actively maintained |
| **Best performance needed** | Bayesian Optimization | Superior results, worth extra time |
| **Limited time budget** | TPE (Hyperopt) | Best speed/quality tradeoff |
| **Small discrete space** | Grid Search | Guarantees finding optimum |
| **Research paper** | Bayesian + TPE | Multiple strong baselines |

### Algorithm Details

| Algorithm | How It Works | Strengths | Limitations |
|-----------|--------------|-----------|-------------|
| **Grid Search** | Exhaustive evaluation of all combinations | Complete coverage, reproducible | Exponential complexity |
| **Random Search** | Random sampling from distributions | Fast, handles continuous params | No learning between trials |
| **Optuna** | TPE with automatic pruning | Modern, efficient, production-ready | Requires setup |
| **Bayesian Optimization** | Gaussian process model of objective | Intelligent search, best results | Slower initial phase |
| **TPE** | Tree-structured Parzen estimators | Fast convergence, proven reliability | Fewer features than Optuna |

---

## ğŸ† Benchmark Details

### Experimental Setup

- **Dataset**: Sklearn Diabetes (442 samples, 10 features)
- **Model**: LightGBM Regressor
- **Iterations**: 50 trials per method
- **Validation**: 2-fold cross-validation
- **Metric**: Mean Squared Error (lower is better)

### Performance Characteristics

| Algorithm | Speed | Consistency | Typical Improvement |
|-----------|-------|-------------|---------------------|
| **TPE (Hyperopt)** | âš¡âš¡âš¡ Fastest | High | 25-35% |
| **Optuna** | âš¡âš¡âš¡ Very Fast | High | 20-30% |
| **Random Search** | âš¡âš¡ Fast | Medium | 20-30% |
| **Bayesian Opt** | âš¡ Moderate | High | 20-30% |
| **Grid Search** | âŒ Slow | Very High | 15-25% |

> âš ï¸ **Note**: Values shown are from recent benchmark run. Absolute MSE values vary by environment and random_state, but the ranking and relative performance are consistent across runs.

---

## ğŸ“ Project Structure

```
Hyperparameters-Optimization/
â”œâ”€â”€ HyperParameterInspect.ipynb           # Korean tutorial notebook
â”œâ”€â”€ HyperParameterInspect_EN.ipynb        # English tutorial notebook
â”œâ”€â”€ benchmark_hpo_algorithms.py           # Automated benchmark script
â”œâ”€â”€ requirements.txt                      # Python dependencies
â”œâ”€â”€ README.md                             # This file
â”œâ”€â”€ pic/                                  # Images and plots
â””â”€â”€ doc/                                  # Additional documentation
```

---

## ğŸ”§ Requirements

**Core Dependencies**
- Python 3.8+
- numpy, pandas, scikit-learn, lightgbm

**Optimization Libraries**
- optuna >= 3.0.0 (Modern HPO with pruning)
- hyperopt >= 0.2.7 (TPE algorithm)
- scikit-optimize >= 0.9.0 (Bayesian optimization)

**Visualization**
- matplotlib, jupyter

> âš ï¸ **Important**: This project uses **Optuna** instead of the deprecated `scikit-hyperband` library due to compatibility issues with modern scikit-learn versions.

---

## ğŸ“š References

### Key Papers

- **Random Search**: [Bergstra & Bengio, JMLR 2012](https://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf)
- **TPE**: [Bergstra et al., NIPS 2011](https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf)
- **Bayesian Optimization**: [Snoek et al., 2012](https://arxiv.org/abs/1206.2944)
- **HyperBand**: [Li et al., ICLR 2018](https://arxiv.org/pdf/1603.06560.pdf)

### Presentations

- ğŸ¤ 2022 AI & Big Data Day - "AI Model Hyperparameter Optimization"
- ğŸ¤ 2020 AI Innovation Conference - Hyperparameter Optimization Techniques
- ğŸ“° Featured in [IT Daily](http://www.itdaily.kr/news/articleView.html?idxno=210339), [ComWorld](https://www.comworld.co.kr/news/articleView.html?idxno=50677)

---

## ğŸ¤ Contributing

Contributions welcome! Ways to help:

- ğŸ› Report bugs or issues
- ğŸ’¡ Suggest new features or algorithms
- ğŸ“ Improve documentation
- ğŸŒ Translate to other languages
- ğŸ”¬ Add optimization methods

**Development Setup**
```bash
git clone https://github.com/YOUR_USERNAME/Hyperparameters-Optimization.git
cd Hyperparameters-Optimization
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
python benchmark_hpo_algorithms.py
```

---

##  License

MIT License - see [LICENSE](LICENSE) file for details.

---

## ğŸ‘¤ Author

**Hyeonsang Jeon**  
GitHub: [@hyeonsangjeon](https://github.com/hyeonsangjeon)

---

## ğŸ™ Acknowledgments

Special thanks to:
- [Optuna](https://github.com/optuna/optuna) - Modern HPO framework
- [Hyperopt](https://github.com/hyperopt/hyperopt) - TPE implementation
- [scikit-optimize](https://github.com/scikit-optimize/scikit-optimize) - Bayesian optimization
- [LightGBM](https://github.com/microsoft/LightGBM) - Fast gradient boosting

---

## ğŸ”— Related Projects

- **[Optuna](https://github.com/optuna/optuna)** - Next-generation HPO framework
- **[Hyperopt](https://github.com/hyperopt/hyperopt)** - Distributed HPO library
- **[scikit-optimize](https://github.com/scikit-optimize/scikit-optimize)** - Bayesian optimization
- **[Ray Tune](https://github.com/ray-project/ray)** - Scalable distributed tuning

---

<div align="center">

## â­ Found this helpful?

**Star this repository** to support the project and help others discover it!

### ğŸš€ Share with your team

This tutorial is actively maintained and regularly updated with new techniques.

**Made with â¤ï¸ for the ML community**

[â¬† Back to Top](#-hyperparameter-optimization-tutorial)

</div>
