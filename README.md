![screenshot1](https://github.com/hyeonsangjeon/Hyperparameters-Optimization/blob/master/pic/Turbo-Snail-Turbo-Funny-Black-Silver.jpg?raw=true)
- í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì¡°ì •ì€ ëª¨ë“  ê¸°ê³„ í•™ìŠµ í”„ë¡œì íŠ¸ì˜ í•„ìˆ˜ ë¶€ë¶„ì´ë©° ê°€ì¥ ì‹œê°„ì´ ë§ì´ ê±¸ë¦¬ëŠ” ì‘ì—… ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. 
- ê°€ì¥ ë‹¨ìˆœí•œ ëª¨ë¸ì˜ ê²½ìš°ì—ë„ í•˜ë£¨, ëª‡ ì£¼ ë˜ëŠ” ê·¸ ì´ìƒ ìµœì í™” í•  ìˆ˜ìˆëŠ” ì‹ ê²½ë§ì„ ì–¸ê¸‰í•˜ì§€ ì•Šê³  ìµœì ì˜ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ì°¾ëŠ” ë° ëª‡ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
- ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” Grid Search , Random Search, HyperBand, Bayesian optimization, Tree-structured Parzen Estimator(TPE)ì— ëŒ€í•´ ì†Œê°œí•©ë‹ˆë‹¤. 
- í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì¡°ì •ì€ í•¨ìˆ˜ ìµœì í™” ì‘ì—…ì— ì§€ë‚˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ë¶„ëª…íˆ Grid ë˜ëŠ” Random Searchê°€ ìœ ì¼í•˜ê³  ìµœìƒì˜ ì•Œê³ ë¦¬ì¦˜ì€ ì•„ë‹ˆì§€ë§Œ íš¨ìœ¨ì ì¸ ì†ë„ì™€ ê²°ê³¼ ì¸¡ë©´ì—ì„œ ê¾¸ì¤€íˆ ì‚¬ìš©ë©ë‹ˆë‹¤. 
- ì´ë¡ ì  ê´€ì ì—ì„œ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ì„¤ëª…í•˜ê³  Hyperopt ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤„ ê²ƒì…ë‹ˆë‹¤.
- ì´ íŠœí† ë¦¬ì–¼ì„ ë§ˆì¹˜ë©´ ëª¨ë¸ë§ í”„ë¡œì„¸ìŠ¤ì˜ ì†ë„ë¥¼ ì‰½ê²Œ ë†’ì´ëŠ” ë°©ë²•ì„ ì•Œê²Œë©ë‹ˆë‹¤. 
- íŠœí† ë¦¬ì–¼ì˜ ìœ ì¼í•œ ëª©í‘œëŠ” ë§¤ìš° ë‹¨ìˆœí™” ëœ ì˜ˆì œì—ì„œ Hyperparameter optimizationì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì‹œì—°í•˜ê³  ì„¤ëª…í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.


```python
#!pip install pip install git+https://github.com/darenr/scikit-optimize
```

## Preparation step
- í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ê°€ì ¸ ì˜µë‹ˆë‹¤



```python
#!pip install lightgbm
import numpy as np
import pandas as pd

from lightgbm.sklearn import LGBMRegressor
from sklearn.metrics import mean_squared_error

%matplotlib inline

import warnings                                  # `do not disturbe` mode
warnings.filterwarnings('ignore')
```

#### sklearn.datasetsì˜ ë‹¹ë‡¨ë³‘ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ì„ ì‹œì—°í•˜ê³  ë¹„êµí•©ë‹ˆë‹¤. ë¡œë“œí•©ì‹œë‹¤.

ì—¬ê¸°ì—ì„œ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ ì„¤ëª…ì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. [https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html] 
add: ì¼ë¶€ í™˜ì ë° ëŒ€ìƒ ì¸¡ì • í•­ëª©ì— ëŒ€í•œ ì •ë³´ê°€ í¬í•¨ ëœ ë°ì´í„° ì„¸íŠ¸ì…ë‹ˆë‹¤. "ê¸°ì¤€ì„  1 ë…„ í›„ ì§ˆë³‘ ì§„í–‰ì˜ ì •ëŸ‰ì  ì¸¡ì •". 
ì´ ì˜ˆì œì˜ ëª©ì ì„ ìœ„í•´ ë°ì´í„°ë¥¼ ì´í•´í•  í•„ìš”ë„ ì—†ìŠµë‹ˆë‹¤. íšŒê·€ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  ìˆìœ¼ë©° í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ë ¤ê³ í•œë‹¤ëŠ” ì ì„ ëª…ì‹¬í•˜ì‹­ì‹œì˜¤.


```python
from sklearn.datasets import load_diabetes

diabetes = load_diabetes()
n = diabetes.data.shape[0]

data = diabetes.data
targets = diabetes.target
```

- ë°ì´í„° ì„¸íŠ¸ëŠ” ë§¤ìš° ì‘ìŠµë‹ˆë‹¤. 
- ì´ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ë³¸ ê°œë…ì„ ì‰½ê²Œ ë³´ì—¬ì¤„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì„ íƒí–ˆìŠµë‹ˆë‹¤.( ëª¨ë“  ê²ƒì´ ê³„ì‚° ë  ë•Œ ëª‡ ì‹œê°„ì„ ê¸°ë‹¤ë¦´ í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.) 
- ë°ì´í„° ì„¸íŠ¸ë¥¼ í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆŒ ê²ƒì…ë‹ˆë‹¤. train ë¶€ë¶„ì€ 2 ê°œë¡œ ë¶„í• ë˜ë©°, ë§¤ê°œ ë³€ìˆ˜ë¥¼ ìµœì í™”í•˜ëŠ” ë° ë”°ë¼ êµì°¨ ê²€ì¦ MSEë¥¼ ìµœì¢… ì¸¡ì • í•­ëª©ìœ¼ë¡œ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤. 

add :ì´ ê°„ë‹¨í•œ ì˜ˆì œëŠ” ì‹¤ì œ ëª¨ë¸ë§ì„ ìœ„í•œ ë°©ë²•ì´ ì•„ë‹™ë‹ˆë‹¤. ë¹ ë¥¸ ë°ëª¨ ì†Œê°œì—ë§Œ ì‚¬ìš©í•˜ëŠ” ì‘ì€ ë°ì´í„° ì„¸íŠ¸ì™€ 2 ê°œì˜ foldë¡œ ì¸í•´ ë¶ˆì•ˆì • í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê²°ê³¼ëŠ” random_stateì— ë”°ë¼ í¬ê²Œ ë³€ê²½ë©ë‹ˆë‹¤.

iterationì€ 50ìœ¼ë¡œ ê³ ì •í•©ë‹ˆë‹¤.



```python
from sklearn.model_selection import KFold, cross_val_score
from sklearn.model_selection import train_test_split

random_state=42
n_iter=50

train_data, test_data, train_targets, test_targets = train_test_split(data, targets, 
                                                                      test_size=0.20, shuffle=True,
                                                                      random_state=random_state)
num_folds=2
kf = KFold(n_splits=num_folds, random_state=random_state)
```


```python
print('train_data : ',train_data.shape)
print('test_data : ',test_data.shape)

print('train_targets : ',train_targets.shape)
print('test_targets : ',test_targets.shape)
```

    train_data :  (353, 10)
    test_data :  (89, 10)
    train_targets :  (353,)
    test_targets :  (89,)


### ëª¨ë¸ìƒì„±

LGBMRegressorë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•´ë´…ë‹ˆë‹¤. Gradient Boostingì—ëŠ” ìµœì í™” í•  ìˆ˜ìˆëŠ” ë§ì€ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ê°€ ìˆìœ¼ë¯€ë¡œ ë°ëª¨ì— ì í•©í•œ ì„ íƒì…ë‹ˆë‹¤.


```python
model = LGBMRegressor(random_state=random_state)

```

ê¸°ë³¸ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì¤€ ëª¨ë¸ì„ í•™ìŠµ í•´ ë³´ê² ìŠµë‹ˆë‹¤.

- ì¦‰ì‹œ ì¶œë ¥ëœ ëª¨ë¸ì˜ ê²°ê³¼ëŠ” 3532ì…ë‹ˆë‹¤. 


```python
%%time
score = -cross_val_score(model, train_data, train_targets, cv=kf, scoring="neg_mean_squared_error", n_jobs=-1).mean()
print(score)
```

    3532.0822189641976
    CPU times: user 23 ms, sys: 33 ms, total: 56 ms
    Wall time: 806 ms


#### ì‹¤í—˜ì— ì‚¬ìš©í•œ Scikit-Learnì˜ íŒŒë¼ë¯¸í„°ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. 
- base_estimator: ê¸°ë³¸ ëª¨í˜•
- n_estimators: ëª¨í˜• ê°¯ìˆ˜. ë””í´íŠ¸ 10
- bootstrap: ë°ì´í„°ì˜ ì¤‘ë³µ ì‚¬ìš© ì—¬ë¶€. ë””í´íŠ¸ True
- max_samples: ë°ì´í„° ìƒ˜í”Œ ì¤‘ ì„ íƒí•  ìƒ˜í”Œì˜ ìˆ˜ í˜¹ì€ ë¹„ìœ¨. ë””í´íŠ¸ 1.0
- bootstrap_features: íŠ¹ì§• ì°¨ì›ì˜ ì¤‘ë³µ ì‚¬ìš© ì—¬ë¶€. ë””í´íŠ¸ False
- max_features: ë‹¤ì°¨ì› ë…ë¦½ ë³€ìˆ˜ ì¤‘ ì„ íƒí•  ì°¨ì›ì˜ ìˆ˜ í˜¹ì€ ë¹„ìœ¨ 1.0


#### ìµœì í™” ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ë°ëª¨ ëª©ì ìœ¼ë¡œ 3 ê°œì˜ ë§¤ê°œ ë³€ìˆ˜ ë§Œ ì¡°ì •í•˜ëŠ” ëª¨ë¸ì„ ìµœì í™” í•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤. 
- n_estimators: from 100 to 2000
- max_depth: from 2 to 20
- learning_rate: from 10e-5 to 1


#### Computing powerëŠ” ì¼ë°˜ì ì¸ ë¡œì»¬ ë¯¸ë‹ˆì„œë²„ì™€ í´ë¼ìš°ë“œì»´í“¨íŒ… í™˜ê²½ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. 
- ë¡œì»¬ë¯¸ë‹ˆì„œë²„ : AMD 2700x (1 CPU - 8Core)
- í´ë¼ìš°ë“œì„œë²„ : Accuinsight+ modeler (18 CPU- 162core) [Intel(R) Xeon(R) 2.00GHZ]

# 1. GridSearch


í•˜ì´í¼ íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì „í†µì ì¸ ë°©ë²•ì€ ê·¸ë¦¬ë“œ ê²€ìƒ‰ ë˜ëŠ” ë§¤ê°œ ë³€ìˆ˜ ìŠ¤ìœ•ìœ¼ë¡œ, í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ê³µê°„ì—ì„œ ìˆ˜ë™ìœ¼ë¡œ ì§€ì •ëœ í•˜ìœ„ ì§‘í•©ì„ í†µí•´ ì „ì²´ì ìœ¼ë¡œ ê²€ìƒ‰í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. 

- ê°€ì¥ ë¨¼ì € ì‹œë„í•´ ë³¼ ìˆ˜ìˆëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ sklearn.model_selectionì— í¬í•¨ ëœ GridSearchCVì…ë‹ˆë‹¤.ì´ ì ‘ê·¼ ë°©ì‹ì€ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ë§¤ê°œ ë³€ìˆ˜ì˜ ì¡°í•©ì„ 1 x 1ë¡œ ì‹œë„í•˜ê³  ìµœìƒì˜ êµì°¨ ê²€ì¦ ê²°ê³¼ë¥¼ ê°€ì§„ ê²ƒì„ ì„ íƒí•©ë‹ˆë‹¤.


ì´ ì ‘ê·¼ ë°©ì‹ì—ëŠ” ëª‡ ê°€ì§€ ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤.

1. ë§¤ìš° ëŠë¦½ë‹ˆë‹¤. ëª¨ë“  ë§¤ê°œ ë³€ìˆ˜ì˜ ëª¨ë“  ì¡°í•©ì„ ì‹œë„í•˜ê³  ë§ì€ ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤. ë³€ëŸ‰ í•  ì¶”ê°€ ë§¤ê°œ ë³€ìˆ˜ëŠ” ì™„ë£Œí•´ì•¼í•˜ëŠ” ë°˜ë³µ íšŸìˆ˜ë¥¼ ê³±í•©ë‹ˆë‹¤. ê°€ëŠ¥í•œ ê°’ì´ 10 ê°œì¸ ìƒˆ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ë§¤ê°œ ë³€ìˆ˜ ê·¸ë¦¬ë“œì— ì¶”ê°€í•œë‹¤ê³  ê°€ì • í•´ë³´ì‹­ì‹œì˜¤.ì´ ë§¤ê°œ ë³€ìˆ˜ëŠ” ë¬´ì˜ë¯¸í•œ ê²ƒìœ¼ë¡œ íŒëª… ë  ìˆ˜ ìˆì§€ë§Œ ê³„ì‚° ì‹œê°„ì€ 10 ë°° ì¦ê°€í•©ë‹ˆë‹¤. 
2. ì´ì‚° ê°’ìœ¼ë¡œ ë§Œ ì‘ë™ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì „ì—­ ìµœì  ê°’ì´ n_estimators = 550ì´ì§€ë§Œ 100 ë‹¨ê³„ì—ì„œ 100ì—ì„œ 1000ê¹Œì§€ GridSearchCVë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²½ìš° ìµœì  ì ì— ë„ë‹¬í•˜ì§€ ëª»í•  ê²ƒì…ë‹ˆë‹¤. 
3. ì ì ˆí•œ ì‹œê°„ì— ê²€ìƒ‰ì„ ì™„ë£Œí•˜ë ¤ë©´ approximate localization of the optimumë¥¼ ì•Œê³  / ì¶”ì¸¡í•´ì•¼í•©ë‹ˆë‹¤.

ì´ëŸ¬í•œ ë‹¨ì  ì¤‘ ì¼ë¶€ë¥¼ ê·¹ë³µ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§¤ê°œ ë³€ìˆ˜ë³„ë¡œ ê·¸ë¦¬ë“œ ê²€ìƒ‰ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ìˆ˜í–‰í•˜ê±°ë‚˜ í° ë‹¨ê³„ê°€ìˆëŠ” ë„“ì€ ê·¸ë¦¬ë“œì—ì„œ ì‹œì‘í•˜ì—¬ ì—¬ëŸ¬ ë²ˆ ì‚¬ìš©í•˜ê³  ë°˜ë³µì—ì„œ ê²½ê³„ë¥¼ ì¢íˆê³  ë‹¨ê³„ í¬ê¸°ë¥¼ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, ì—¬ì „íˆ ë§¤ìš° ê³„ì‚° ì§‘ì•½ì ì´ê³  ê¸¸ ê²ƒì…ë‹ˆë‹¤.

- ìš°ë¦¬ì˜ ê²½ìš° ê·¸ë¦¬ë“œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” ë° ê±¸ë¦¬ëŠ” ì‹œê°„ì„ ì¶”ì • í•´ ë³´ê² ìŠµë‹ˆë‹¤. 
> - ê·¸ë¦¬ë“œê°€ 'n_estimators'(100 ~ 2000)ì˜ ê°€ëŠ¥í•œ ê°’ 20 ê°œ, 
> - 'max_depth'ì˜ 19 ê°œ ê°’ (2 ~ 20), 
> - 'learning_rate'(10e-4 ~ 0.1)ì˜ 5 ê°œ ê°’ìœ¼ë¡œ êµ¬ì„±ë˜ê¸°ë¥¼ ì›í•œë‹¤ê³  ê°€ì • í•´ ë³´ê² ìŠµë‹ˆë‹¤.
- ì¦‰, cross_val_score 20 * 19 * 5 = 1900 ë²ˆ ê³„ì‚°í•´ì•¼í•©ë‹ˆë‹¤. 1 ë²ˆ ê³„ì‚°ì— 0.5 ~ 1.0 ì´ˆê°€ ê±¸ë¦¬ë©´ ê·¸ë¦¬ë“œ ê²€ìƒ‰ì€ 15 ~ 30 ë¶„ ë™ì•ˆ ì§€ì†ë©ë‹ˆë‹¤. ~ 400 ë°ì´í„° í¬ì¸íŠ¸ê°€ìˆëŠ” ë°ì´í„° ì„¸íŠ¸ì—ëŠ” ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤.
- ì‹¤í—˜ ì‹œê°„ì€ ì˜¤ë˜ ê±¸ë¦¬ì§€ ë§ì•„ì•¼í•˜ë¯€ë¡œ, ì´ ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë¶„ì„ í•  êµ¬ê°„ì„ ì¢í˜€ ì•¼í•©ë‹ˆë‹¤. 5 * 8 * 3 = 120 ì¡°í•© ë§Œ ë‚¨ê²¼ìŠµë‹ˆë‹¤.      
> - Accuinsight+ modeler (18 CPU- 162core) Wall time: 5.5 s
> - AMD 2700x (1 CPU - 8Core) Wall time: 6.7 s 


```python
%%time
from sklearn.model_selection import GridSearchCV

param_grid={'max_depth':  np.linspace(5,12,8,dtype = int),
            'n_estimators': np.linspace(800,1200,5, dtype = int),
            'learning_rate': np.logspace(-3, -1, 3),            
            'random_state': [random_state]}

gs=GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', n_jobs=-1, cv=kf, verbose=False)

gs.fit(train_data, train_targets)
gs_test_score=mean_squared_error(test_targets, gs.predict(test_data))


print('===========================')
print("Best MSE = {:.3f} , when params {}".format(-gs.best_score_, gs.best_params_))
print('===========================')
```

    ===========================
    Best MSE = 3319.975 , when params {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 800, 'random_state': 42}
    ===========================
    CPU times: user 1.58 s, sys: 21 ms, total: 1.6 s
    Wall time: 6.3 s


ê²°ê³¼ë¥¼ ê°œì„ í–ˆì§€ë§Œ, ê·¸ê²ƒì— ë§ì€ ì‹œê°„ì„ ë³´ëƒˆìŠµë‹ˆë‹¤. ë§¤ê°œ ë³€ìˆ˜ê°€ ë°˜ë³µì—ì„œ ë°˜ë³µìœ¼ë¡œ ì–´ë–»ê²Œ ë³€ê²½ë˜ì—ˆëŠ”ì§€ ì‚´í´ ë³´ê² ìŠµë‹ˆë‹¤.

- ì•„ë˜ ê·¸ë¦¼ì—ì„œ, (MSEê°€ ë‚®ì„ë•Œ ê° ë³€ìˆ˜ê´€ê³„ ì°¸ì¡°)
> - ì˜ˆë¥¼ ë“¤ì–´ max_depthëŠ” ì ìˆ˜ì— í¬ê²Œ ì˜í–¥ì„ì£¼ì§€ ì•ŠëŠ” ê°€ì¥ ëœ ì¤‘ìš”í•œ ë§¤ê°œ ë³€ìˆ˜ì„ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ìš°ë¦¬ëŠ” max_depthì˜ 8 ê°€ì§€ ë‹¤ë¥¸ ê°’ì„ ê²€ìƒ‰í•˜ê³  ë‹¤ë¥¸ ë§¤ê°œ ë³€ìˆ˜ì— ëŒ€í•œ ê³ ì • ê°’ ê²€ìƒ‰ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì‹œê°„ê³¼ ìì›ì˜ ë‚­ë¹„ì…ë‹ˆë‹¤. 


```python
gs_results_df=pd.DataFrame(np.transpose([-gs.cv_results_['mean_test_score'],
                                         gs.cv_results_['param_learning_rate'].data,
                                         gs.cv_results_['param_max_depth'].data,
                                         gs.cv_results_['param_n_estimators'].data]),
                           columns=['score', 'learning_rate', 'max_depth', 'n_estimators'])
gs_results_df.plot(subplots=True,figsize=(10, 10))
```




    array([<matplotlib.axes._subplots.AxesSubplot object at 0x7fbdc6fc7898>,
           <matplotlib.axes._subplots.AxesSubplot object at 0x7fbdc6febe80>,
           <matplotlib.axes._subplots.AxesSubplot object at 0x7fbdc6f96898>,
           <matplotlib.axes._subplots.AxesSubplot object at 0x7fbdc6fd2828>],
          dtype=object)




![png](./pic/output_17_1.png)


# 2. Random Search

### Research Paper [Random Search](https://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf)

- Random SearchëŠ” ê·¸ë¦¬ë“œ ê²€ìƒ‰ë³´ë‹¤ í‰ê· ì ìœ¼ë¡œ ë” íš¨ê³¼ì ì…ë‹ˆë‹¤.


<img src="https://raw.githubusercontent.com/nslatysheva/data_science_blogging/master/expanding_ML_toolkit/expanding_toolkit.jpg" style="height:500px;width:50%;"/>

#### ì£¼ìš” ì¥ì  : 
1. ì˜ë¯¸ì—†ëŠ” ë§¤ê°œ ë³€ìˆ˜ì— ì‹œê°„ì„ ì†Œë¹„í•˜ì§€ ì•‰ìŒ. ëª¨ë“  ë‹¨ê³„ì—ì„œ ë¬´ì‘ìœ„ ê²€ìƒ‰ì€ ëª¨ë“  ë§¤ê°œ ë³€ìˆ˜ë¥¼ ë³€ê²½í•©ë‹ˆë‹¤. 
2. í‰ê· ì ìœ¼ë¡œ ê·¸ë¦¬ë“œ ê²€ìƒ‰ë³´ë‹¤ í›¨ì”¬ ë¹ ë¥´ê²Œ ~ ìµœì ì˜ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤. 
3. ì—°ì† ë§¤ê°œ ë³€ìˆ˜ë¥¼ ìµœì í™” í•  ë•Œ ê·¸ë¦¬ë“œì— ì˜í•´ ì œí•œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

#### ë‹¨ì : 
1. ê·¸ë¦¬ë“œì—ì„œ ê¸€ë¡œë²Œ ìµœì  ë§¤ê°œ ë³€ìˆ˜ë¥¼ ì°¾ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
2. ëª¨ë“  ë‹¨ê³„ëŠ” ë…ë¦½ì ì…ë‹ˆë‹¤. ëª¨ë“  íŠ¹ì • ë‹¨ê³„ì—ì„œ ì§€ê¸ˆê¹Œì§€ ìˆ˜ì§‘ ëœ ê²°ê³¼ì— ëŒ€í•œ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 

ì˜ˆì œëŠ”, sklearn.model_selectionì—ì„œ RandomizedSearchCVë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
ë§¤ìš° ë„“ì€ ë§¤ê°œ ë³€ìˆ˜ ê³µê°„ìœ¼ë¡œ ì‹œì‘í•˜ì—¬ 50 ê°œì˜ ë¬´ì‘ìœ„ ë‹¨ê³„ ë§Œ ë§Œë“¤ ê²ƒì…ë‹ˆë‹¤.

ìˆ˜í–‰ì†ë„:
> - Accuinsight+ modeler (18 CPU- 162core) Wall time: 2.51 s
> - AMD 2700x (1 CPU - 8Core) Wall time: 3.08 s 


```python
%%time
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

param_grid_rand={'learning_rate': np.logspace(-5, 0, 100),
                 'max_depth':  randint(2,20),
                 'n_estimators': randint(100,2000),
                 'random_state': [random_state]}

rs=RandomizedSearchCV(model, param_grid_rand, n_iter = n_iter, scoring='neg_mean_squared_error',
                n_jobs=-1, cv=kf, verbose=False, random_state=random_state)

rs.fit(train_data, train_targets)

rs_test_score=mean_squared_error(test_targets, rs.predict(test_data))

print('===========================')
print("Best MSE = {:.3f} , when params {}".format(-rs.best_score_, rs.best_params_))
print('===========================')
```

    ===========================
    Best MSE = 3200.402 , when params {'learning_rate': 0.0047508101621027985, 'max_depth': 19, 'n_estimators': 829, 'random_state': 42}
    ===========================
    CPU times: user 1.16 s, sys: 25 ms, total: 1.19 s
    Wall time: 3.15 s


ê²°ê³¼ëŠ” GridSearchCVë³´ë‹¤ ë‚«ìŠµë‹ˆë‹¤. ë” ì ì€ ì‹œê°„ì„ ì†Œë¹„í•˜ê³  ë” ì™„ì „í•œ ê²€ìƒ‰ì„í–ˆìŠµë‹ˆë‹¤. ì‹œê°í™”ë¥¼ ì‚´í´ ë³´ê² ìŠµë‹ˆë‹¤.
- random searchì˜ ëª¨ë“  ë‹¨ê³„ëŠ” ì™„ì „íˆ ë¬´ì‘ìœ„ì…ë‹ˆë‹¤. ì“¸ëª¨ì—†ëŠ” ë§¤ê°œ ë³€ìˆ˜ì— ì‹œê°„ì„ ì†Œë¹„í•˜ì§€ ì•ŠëŠ” ë° ë„ì›€ì´ë˜ì§€ë§Œ ì—¬ì „íˆ ì²« ë²ˆì§¸ ë‹¨ê³„ì—ì„œ ìˆ˜ì§‘ ëœ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ í›„ìì˜ ê²°ê³¼ë¥¼ ê°œì„ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.


```python
rs_results_df=pd.DataFrame(np.transpose([-rs.cv_results_['mean_test_score'],
                                         rs.cv_results_['param_learning_rate'].data,
                                         rs.cv_results_['param_max_depth'].data,
                                         rs.cv_results_['param_n_estimators'].data]),
                           columns=['score', 'learning_rate', 'max_depth', 'n_estimators'])
rs_results_df.plot(subplots=True,figsize=(10, 10))
```




    array([<matplotlib.axes._subplots.AxesSubplot object at 0x7fbdc6dfd4a8>,
           <matplotlib.axes._subplots.AxesSubplot object at 0x7fbdc6d44668>,
           <matplotlib.axes._subplots.AxesSubplot object at 0x7fbdc6d78630>,
           <matplotlib.axes._subplots.AxesSubplot object at 0x7fbdc6d2c630>],
          dtype=object)




![png](./pic/output_22_1.png)



## 3. HyperBand

### Research Paper [HyperBand](https://arxiv.org/pdf/1603.06560.pdf)
Abstract ë°œì·Œ:
ë¨¸ì‹  ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ì€ ì¢‹ì€ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì§‘í•©ì„ ì‹ë³„í•˜ëŠ” ë° ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤. ìµœê·¼ ì ‘ê·¼ ë°©ì‹ì€ ë² ì´ì§€ì•ˆ ìµœì í™”ë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬ì„±ì„ ì ì‘ ì ìœ¼ë¡œ ì„ íƒí•˜ì§€ë§Œ ì ì‘ í˜• ë¦¬ì†ŒìŠ¤ í• ë‹¹ ë° ì¡°ê¸° ì¤‘ì§€ë¥¼ í†µí•´ ì„ì˜ ê²€ìƒ‰ ì†ë„ë¥¼ ë†’ì´ëŠ” ë° ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤. ë°˜ë³µ, ë°ì´í„° ìƒ˜í”Œ ë˜ëŠ” ê¸°ëŠ¥ê³¼ ê°™ì€ ì‚¬ì „ ì •ì˜ ëœ ë¦¬ì†ŒìŠ¤ê°€ ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œë§ ëœ êµ¬ì„±ì— í• ë‹¹ë˜ëŠ” ìˆœìˆ˜ íƒìƒ‰ ë¹„ í™•ë¥  ì  ë¬´í•œ ë¬´ì¥ ë°´ë””íŠ¸ ë¬¸ì œë¡œ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ê³µì‹í™”í•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ ì›Œí¬ì— ëŒ€í•´ ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ ì¸ Hyperbandë¥¼ ë„ì…í•˜ê³  ì´ë¡ ì  ì†ì„±ì„ ë¶„ì„í•˜ì—¬ ëª‡ ê°€ì§€ ë°”ëŒì§í•œ ë³´ì¥ì„ ì œê³µí•©ë‹ˆë‹¤. ë˜í•œ, í•˜ì´í¼ íŒŒë¼ë¯¸í„° ìµœì í™” ë¬¸ì œ ëª¨ìŒì— ëŒ€í•´ Hyperbandë¥¼ ì¸ê¸°ìˆëŠ” ë² ì´ì§€ì•ˆ ìµœì í™” ë°©ë²•ê³¼ ë¹„êµí•©ë‹ˆë‹¤. HyperbandëŠ” ë‹¤ì–‘í•œ ë”¥ ëŸ¬ë‹ ë° ì»¤ë„ ê¸°ë°˜ í•™ìŠµ ë¬¸ì œì— ëŒ€í•´ ê²½ìŸ ì—…ì²´ë³´ë‹¤ í›¨ì”¬ ë¹ ë¥¸ ì†ë„ë¥¼ ì œê³µ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Â© 2018 Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh ë° Ameet Talwalkar.


<img src="https://github.com/hyeonsangjeon/Hyperparameters-Optimization/blob/master/pic/Hyperband.png?raw=true" />

- Hyperband SearchëŠ” ìµœì í™” ê²€ìƒ‰ ì†ë„ì— ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤. 
- nê°œì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ì„ ëœë¤ ìƒ˜í”Œë§.
- ì „ì²´ resourceë¥¼ nê°œë¡œ ë¶„í• í•˜ê³ , í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì¡°í•©ì— ê°ê° í• ë‹¹í•˜ì—¬ í•™ìŠµ
- ê° í•™ìŠµ í”„ë¡œì„¸ìŠ¤ëŠ” ì¼ì • ë¹„ìœ¨ ì´ìƒì˜ ìƒìœ„ ì¡°í•©ì„ ë‚¨ê¸°ê³  ë²„ë¦¼. 


ìˆ˜í–‰ì†ë„:
> - Accuinsight+ modeler (18 CPU- 162core) Wall time: 2.51 s
> - AMD 2700x (1 CPU - 8Core) Wall time: 1.19 s 

cloudìì›ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ë¶„ì‚° ìì›ì˜ ì¤€ë¹„ ì‹œê°„ì´ ìƒëŒ€ì ìœ¼ë¡œ ê¸´ê²ƒì„ ë³¼ìˆ˜ ìˆì—ˆìŒ. 


```python
!git clone https://github.com/thuijskens/scikit-hyperband.git 2>/dev/null 1>/dev/null
```


```python
!cp -r scikit-hyperband/* .
```


```python
!python setup.py install 2>/dev/null 1>/dev/null
```


```python
%%time
from hyperband import HyperbandSearchCV

from scipy.stats import randint as sp_randint
from sklearn.preprocessing import LabelBinarizer


param_hyper_band={'learning_rate': np.logspace(-5, 0, 100),
                 'max_depth':  randint(2,20),
                 'n_estimators': randint(100,2000),                  
                 #'num_leaves' : randint(2,20),
                 'random_state': [random_state]
                 }


hb = HyperbandSearchCV(model, param_hyper_band, max_iter = n_iter, scoring='neg_mean_squared_error', resource_param='n_estimators', random_state=random_state)


#%time search.fit(new_training_data, y)
hb.fit(train_data, train_targets)



hb_test_score=mean_squared_error(test_targets, hb.predict(test_data))

print('===========================')
print("Best MSE = {:.3f} , when params {}".format(-hb.best_score_, hb.best_params_))
print('===========================')
```

    ===========================
    Best MSE = 3431.685 , when params {'learning_rate': 0.13848863713938717, 'max_depth': 12, 'n_estimators': 16, 'random_state': 42}
    ===========================
    CPU times: user 13.4 s, sys: 64 ms, total: 13.5 s
    Wall time: 2.06 s



```python
hb_results_df=pd.DataFrame(np.transpose([-hb.cv_results_['mean_test_score'],
                                         hb.cv_results_['param_learning_rate'].data,
                                         hb.cv_results_['param_max_depth'].data,
                                         hb.cv_results_['param_n_estimators'].data]),
                           columns=['score', 'learning_rate', 'max_depth', 'n_estimators'])
hb_results_df.plot(subplots=True,figsize=(10, 10))
```




    array([<matplotlib.axes._subplots.AxesSubplot object at 0x7fbdc6c3f7f0>,
           <matplotlib.axes._subplots.AxesSubplot object at 0x7fbdc6c2a358>,
           <matplotlib.axes._subplots.AxesSubplot object at 0x7fbdc6bd4320>,
           <matplotlib.axes._subplots.AxesSubplot object at 0x7fbdc6b882b0>],
          dtype=object)




![png](./pic/output_28_1.png)


## 4. Bayesian optimization

### Research Paper [Bayesian optimization](https://arxiv.org/pdf/1012.2599.pdf)


Random ë˜ëŠ” Grid Searchì™€ ë‹¬ë¦¬ ë² ì´ì§€ì•ˆ ì ‘ê·¼ ë°©ì‹ì€ ëª©í‘œ í•¨ìˆ˜ì˜ ì ìˆ˜ í™•ë¥ ì— í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ë§¤í•‘í•˜ëŠ” í™•ë¥  ëª¨ë¸ì„ í˜•ì„±í•˜ëŠ”ë° ì‚¬ìš©í•˜ëŠ” ê³¼ê±° í‰ê°€ ê²°ê³¼ë¥¼ ì¶”ì í•©ë‹ˆë‹¤.

![](https://github.com/hyeonsangjeon/Hyperparameters-Optimization/blob/master/pic/BayesianOpt.gif?raw=true)

<img src="https://github.com/hyeonsangjeon/Hyperparameters-Optimization/blob/master/pic/bayesopt2.png?raw=true" style="height:320px;"  />





*P(Score | Hyperparameters)*

ë…¼ë¬¸ì—ì„œ ì´ ëª¨ë¸ì€ ëª©ì  í•¨ìˆ˜ì— ëŒ€í•œ "surrogate"ë¼ê³ í•˜ë©° p (y | x)ë¡œ í‘œì‹œë©ë‹ˆë‹¤. surrogate í•¨ìˆ˜ëŠ” ëª©ì  í•¨ìˆ˜ë³´ë‹¤ ìµœì í™”í•˜ê¸° í›¨ì”¬ ì‰¬ìš° ë©° ë² ì´ì§€ì•ˆ ë°©ë²•ì€ ëŒ€ë¦¬ í•¨ìˆ˜ì—ì„œ ê°€ì¥ ì˜ ìˆ˜í–‰ë˜ëŠ” í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì„ íƒí•˜ì—¬ ì‹¤ì œ ëª©ì  í•¨ìˆ˜ë¥¼ í‰ê°€í•  ë‹¤ìŒ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì„¸íŠ¸ë¥¼ ì°¾ëŠ” ë°©ì‹ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤. 

pseudo codeë¡œ ì •ë¦¬í•˜ë©´:
> 1. ëª©ì  í•¨ìˆ˜ì˜ ëŒ€ë¦¬ í™•ë¥  ëª¨ë¸ êµ¬ì¶•
2. surrogateì—ì„œ ê°€ì¥ ì˜ ìˆ˜í–‰ë˜ëŠ” í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
3. ì´ëŸ¬í•œ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì‹¤ì œ ëª©ì  í•¨ìˆ˜ì— ì ìš©
4. ìƒˆ ê²°ê³¼ë¥¼ í†µí•©í•˜ëŠ” ëŒ€ë¦¬ ëª¨ë¸ ì—…ë°ì´íŠ¸
5. ìµœëŒ€ ë°˜ë³µ ë˜ëŠ” ì‹œê°„ì— ë„ë‹¬ í•  ë•Œê¹Œì§€ 2-4 ë‹¨ê³„ë¥¼ ë°˜ë³µí•©ë‹ˆë‹¤.

ë” ê¹Šì´ìˆëŠ” ë² ì´ì§€ì•ˆ ìµœì í™”ì— ëŒ€í•œ í›Œë¥­í•œ ì»¤ë„ì€ ì—¬ê¸° ì°¸ì¡°: https://www.kaggle.com/artgor/bayesian-optimization-for-robots

- Surrogate Model :
í˜„ì¬ê¹Œì§€ ì¡°ì‚¬ëœ ì…ë ¥ê°’-í•¨ìˆ«ê°’ ì ë“¤Â ${(ğ‘¥_1,f(ğ‘¥_1)), ..., (ğ‘¥_ğ‘¡,f(ğ‘¥_ğ‘¡))}$ ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë¯¸ì§€ì˜ ëª©ì  í•¨ìˆ˜ì˜ í˜•íƒœì— ëŒ€í•œ í™•ë¥ ì ì¸ ì¶”ì •ì„ ìˆ˜í–‰í•˜ëŠ” ëª¨ë¸ 

- Acquisition Function:
 ëª©ì  í•¨ìˆ˜ì— ëŒ€í•œ í˜„ì¬ê¹Œì§€ì˜ í™•ë¥ ì  ì¶”ì • ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, â€˜ìµœì  ì…ë ¥ê°’Â ${ğ‘¥^âˆ—}$ë¥¼ ì°¾ëŠ” ë° ìˆì–´ ê°€ì¥ ìœ ìš©í•  ë§Œí•œâ€™ ë‹¤ìŒ ì…ë ¥ê°’ í›„ë³´Â ${ğ‘¥_(ğ‘¡+1)}$ì„ ì¶”ì²œí•´ ì£¼ëŠ” í•¨ìˆ˜
   Expected Improvement(EI) í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤.  
   
   
   
ìˆ˜í–‰ì†ë„:
> - Accuinsight+ modeler (18 CPU- 162core) Wall time:  2min 24s
> - AMD 2700x (1 CPU - 8Core) Wall time: 1min 36s

ìƒëŒ€ì ìœ¼ë¡œ ë¡œì»¬í…ŒìŠ¤íŠ¸ì˜ ìˆ˜í–‰ ì†ë„ê°€ ë¹ ë¥¸ê²ƒì„ ë³¼ìˆ˜ìˆì—ˆë‹¤.


```python
#! pip install scikit-optimize
#https://towardsdatascience.com/hyperparameter-optimization-with-scikit-learn-scikit-opt-and-keras-f13367f3e796
from skopt import BayesSearchCV
from skopt.space import Real, Categorical, Integer

```


```python
%%time

search_space={'learning_rate': np.logspace(-5, 0, 100),
                 "max_depth": Integer(2, 20), 
                 'n_estimators': Integer(100,2000),
                 'random_state': [random_state]}
                 

def on_step(optim_result):
    """
    Callback meant to view scores after
    each iteration while performing Bayesian
    Optimization in Skopt"""
    score = bayes_search.best_score_
    print("best score: %s" % score)
    if score >= 0.98:
        print('Interrupting!')
        return True
    
bayes_search = BayesSearchCV(model, search_space, n_iter=n_iter, # specify how many iterations
                                    scoring='neg_mean_squared_error', n_jobs=-1, cv=5)
bayes_search.fit(train_data, train_targets, callback=on_step) # callback=on_step will print score after each iteration

bayes_test_score=mean_squared_error(test_targets, bayes_search.predict(test_data))

print('===========================')
print("Best MSE = {:.3f} , when params {}".format(-bayes_search.best_score_, bayes_search.best_params_))
print('===========================')
```

    best score: -4415.920614880022
    best score: -4415.920614880022
    best score: -4415.920614880022
    best score: -4415.920614880022
    best score: -4116.905834420919
    best score: -4116.905834420919
    best score: -4116.905834420919
    best score: -4116.905834420919
    best score: -4116.905834420919
    best score: -3540.855689828868
    best score: -3467.4059934906645
    best score: -3467.4059934906645
    best score: -3467.4059934906645
    best score: -3467.4059934906645
    best score: -3467.4059934906645
    best score: -3467.4059934906645
    best score: -3467.4059934906645
    best score: -3467.4059934906645
    best score: -3467.4059934906645
    best score: -3465.869585251784
    best score: -3462.4668073239764
    best score: -3462.4668073239764
    best score: -3462.4668073239764
    best score: -3460.603434822278
    best score: -3460.603434822278
    best score: -3460.603434822278
    best score: -3460.603434822278
    best score: -3460.603434822278
    best score: -3460.603434822278
    best score: -3460.603434822278
    best score: -3460.603434822278
    best score: -3459.5705953392157
    best score: -3456.063877875675
    best score: -3456.063877875675
    best score: -3454.9987003394112
    best score: -3454.9987003394112
    best score: -3454.9987003394112
    best score: -3454.9987003394112
    best score: -3454.9987003394112
    best score: -3454.9987003394112
    best score: -3454.9987003394112
    best score: -3454.9987003394112
    best score: -3454.9987003394112
    best score: -3454.9987003394112
    best score: -3454.9987003394112
    best score: -3454.9987003394112
    best score: -3454.9987003394112
    best score: -3454.9987003394112
    best score: -3454.9987003394112
    best score: -3454.9987003394112
    ===========================
    Best MSE = 3454.999 , when params OrderedDict([('learning_rate', 0.005336699231206307), ('max_depth', 2), ('n_estimators', 655), ('random_state', 42)])
    ===========================
    CPU times: user 1min 59s, sys: 3min 34s, total: 5min 33s
    Wall time: 1min 26s



```python
bayes_results_df=pd.DataFrame(np.transpose([
                                         -np.array(bayes_search.cv_results_['mean_test_score']),
                                         np.array(bayes_search.cv_results_['param_learning_rate']).data,
                                         np.array(bayes_search.cv_results_['param_max_depth']).data,
                                         np.array(bayes_search.cv_results_['param_n_estimators']).data
                                        ]),
                           columns=['score', 'learning_rate', 'max_depth', 'n_estimators'])



bayes_results_df.plot(subplots=True,figsize=(10, 10))
```




    array([<matplotlib.axes._subplots.AxesSubplot object at 0x7fbd6bfcc208>,
           <matplotlib.axes._subplots.AxesSubplot object at 0x7fbd68640470>,
           <matplotlib.axes._subplots.AxesSubplot object at 0x7fbd686b97f0>,
           <matplotlib.axes._subplots.AxesSubplot object at 0x7fbd686f1c50>],
          dtype=object)




![png](./pic/output_32_1.png)


## 5.Hyperopt
- ì´ ì•Œê³ ë¦¬ì¦˜ì„ ë‹¤ë£¨ê¸° ìœ„í•´ hyperopt [https://github.com/hyperopt/hyperopt] ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
- í˜„ì¬, í•˜ì´í¼ íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ìœ„í•œ ìµœì‹  ë¼ì´ë¸ŒëŸ¬ë¦¬ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.


```python
#!pip install hyperopt
```

ìš°ì„  hyperoptì—ì„œ ëª‡ ê°€ì§€ ìœ ìš©í•œ í•¨ìˆ˜ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. 
- fmin :   ìµœì†Œí™” ë©”ì¸ ëª©ì  í•¨ìˆ˜
- tpe and anneal : optimization ì ‘ê·¼ë°©ì‹
- hp : ë‹¤ì–‘í•œ ë³€ìˆ˜ë“¤ì˜ ë¶„í¬ í¬í•¨
- Trials : loggingì— ì‚¬ìš©



```python
from hyperopt import fmin, tpe, hp, anneal, Trials
```

hyperop.fminì˜ ì¸í„°í˜ì´ìŠ¤ëŠ” Grid ë˜ëŠ” Randomized searchì™€ ë‹¤ë¦…ë‹ˆë‹¤. 
ë¨¼ì €, ìµœì†Œí™” í•  í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. 
-  ì•„ë˜ëŠ” 'learning_rate', 'max_depth', 'n_estimators'ì™€ ê°™ì€ ë‹¤ì–‘í•œ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” gb_mse_cv () í•¨ìˆ˜ê°€ ìˆìŠµë‹ˆë‹¤.


```python
def gb_mse_cv(params, random_state=random_state, cv=kf, X=train_data, y=train_targets):
    # the function gets a set of variable parameters in "param"
    params = {'n_estimators': int(params['n_estimators']), 
              'max_depth': int(params['max_depth']), 
             'learning_rate': params['learning_rate']}
    
    # we use this params to create a new LGBM Regressor
    model = LGBMRegressor(random_state=random_state, **params)
    
    # and then conduct the cross validation with the same folds as before
    score = -cross_val_score(model, X, y, cv=cv, scoring="neg_mean_squared_error", n_jobs=-1).mean()

    return score
```

## 5.1 Tree-structured Parzen Estimator(TPE)

### Research Paper [TPE](https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf)


<img src="https://github.com/hyeonsangjeon/Hyperparameters-Optimization/blob/master/pic/TPE.gif?raw=true" />

TPEëŠ” Hyperoptì˜ ê¸°ë³¸ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. ìµœì í™”ë¥¼ ìœ„í•´ ë² ì´ì§€ì•ˆ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ëª¨ë“  ë‹¨ê³„ì—ì„œ í•¨ìˆ˜ì˜ í™•ë¥  ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³  ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ê°€ì¥ ìœ ë§í•œ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ì„ íƒí•˜ë ¤ê³ í•©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì´ëŸ¬í•œ ìœ í˜•ì˜ ì•Œê³ ë¦¬ì¦˜ì€ ë‹¤ìŒê³¼ ê°™ì´ ì‘ë™í•©ë‹ˆë‹¤.
> - 1.ì„ì˜ì˜ initial point ìƒì„± ${x^*}$
> - 2.${F(x^*)}$ ê³„ì‚°
> - 3.trials ë¡œê¹… ì´ë ¥ì„ ì‚¬ìš©í•´ì„œ, ì¡°ê±´ë¶€ í™•ë¥ ëª¨ë¸  $P(F | x)$ë¥¼ ìƒì„±
> - 4.$ P (F | x) $ì— ë”°ë¼ $ {F (x_i)} $ê°€ ë” ë‚˜ì•„ì§ˆ ê°€ëŠ¥ì„±ì´ ê°€ì¥ ë†’ì€ $ {x_i} $ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
> - 5.$ {F (x_i)} $ì˜ real valuesë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
> - 6.ì¤‘ì§€ ê¸°ì¤€ ì¤‘ í•˜ë‚˜ê°€ ì¶©ì¡± ë  ë•Œê¹Œì§€ 3-5 ë‹¨ê³„ë¥¼ ë°˜ë³µí•©ë‹ˆë‹¤ (ì˜ˆ : i> max_eval).

ì˜ˆë¥¼ ë“¤ì–´ íŠ¹ì • TPE ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•œ ìì„¸í•œ ì •ë³´ëŠ” ì•„ë˜ ë§í¬ ì°¸ì¡°. (ì´ ë§í¬ ë‚´ìš©ì€ ìƒì„¸ë²„ì „ìœ¼ë¡œ, íŠœí† ë¦¬ì–¼ì˜ ë²”ìœ„ë¥¼ ë²—ì–´ë‚©ë‹ˆë‹¤.)

[https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f] 

- fminì˜ ì‚¬ìš©ì€ ë§¤ìš° ê°„ë‹¨í•©ë‹ˆë‹¤. ë§¤ê°œ ë³€ìˆ˜ì˜ ê°€ëŠ¥í•œ ê³µê°„ì„ ì •ì˜í•˜ê³  í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ê¸° ë§Œí•˜ë©´ë©ë‹ˆë‹¤.

ìˆ˜í–‰ì†ë„:
> - Accuinsight+ modeler (18 CPU- 162core) Wall time:  7.3s
> - AMD 2700x (1 CPU - 8Core) Wall time: 7.98s


```python
%%time

# possible values of parameters
space={'n_estimators': hp.quniform('n_estimators', 100, 2000, 1),
       'max_depth' : hp.quniform('max_depth', 2, 20, 1),
       'learning_rate': hp.loguniform('learning_rate', -5, 0)
      }

# trials will contain logging information
trials = Trials()

best=fmin(fn=gb_mse_cv, # function to optimize
          space=space, 
          algo=tpe.suggest, # optimization algorithm, hyperotp will select its parameters automatically
          max_evals=n_iter, # maximum number of iterations
          trials=trials, # logging
          rstate=np.random.RandomState(random_state) # fixing random state for the reproducibility
         )

# computing the score on the test set
model = LGBMRegressor(random_state=random_state, n_estimators=int(best['n_estimators']),
                      max_depth=int(best['max_depth']),learning_rate=best['learning_rate'])
model.fit(train_data,train_targets)
tpe_test_score=mean_squared_error(test_targets, model.predict(test_data))

print("Best MSE {:.3f} params {}".format( gb_mse_cv(best), best))
```

    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:06<00:00,  8.32trial/s, best loss: 3186.7910608402444]
    Best MSE 3186.791 params {'learning_rate': 0.026975706032324936, 'max_depth': 20.0, 'n_estimators': 168.0}
    CPU times: user 784 ms, sys: 37 ms, total: 821 ms
    Wall time: 6.08 s


Best MSE 3186ë¡œ RandomizedSearchì— ë¹„í•´ ì‹œê°„ì€ ê±¸ë¦¬ì§€ë§Œ, ì¢€ ë” ë‚˜ì€ ì†”ë£¨ì…˜ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.


```python
tpe_results=np.array([[x['result']['loss'],
                      x['misc']['vals']['learning_rate'][0],
                      x['misc']['vals']['max_depth'][0],
                      x['misc']['vals']['n_estimators'][0]] for x in trials.trials])

tpe_results_df=pd.DataFrame(tpe_results,
                           columns=['score', 'learning_rate', 'max_depth', 'n_estimators'])
tpe_results_df.plot(subplots=True,figsize=(10, 10))
```




    array([<matplotlib.axes._subplots.AxesSubplot object at 0x7fbd5e386c88>,
           <matplotlib.axes._subplots.AxesSubplot object at 0x7fbd5e2f9828>,
           <matplotlib.axes._subplots.AxesSubplot object at 0x7fbd5e3f7828>,
           <matplotlib.axes._subplots.AxesSubplot object at 0x7fbd5e426c88>],
          dtype=object)




![png](./pic/output_43_1.png)


## Results

ëª¨ë“  ì ‘ê·¼ ë°©ì‹ì— ëŒ€í•´ iterations ìˆ˜ì— ë”°ë¥¸ best_cumulative_scoreë¥¼ ì‹œê°í™” í•´ë´…ë‹ˆë‹¤.


```python
scores_df=pd.DataFrame(index=range(n_iter))
scores_df['Grid Search']=gs_results_df['score'].cummin()
scores_df['Random Search']=rs_results_df['score'].cummin()
scores_df['Hyperband']=hb_results_df['score'].cummin()
scores_df['Bayesian optimization ']=bayes_results_df['score'].cummin()
scores_df['TPE']=tpe_results_df['score'].cummin()


ax = scores_df.plot()

ax.set_xlabel("number_of_iterations")
ax.set_ylabel("best_cumulative_score")
```




    Text(0, 0.5, 'best_cumulative_score')




![png](./pic/output_46_1.png)


- Random SearchëŠ” ë‹¨ìˆœí•˜ë©´ì„œ, ì‹œê°„ì˜ ë¹„ìš©ì— ë”°ë¥¸ ìŠ¤ì½”ì–´ê°€ ë†’ì€ ê²ƒì„ ì•Œ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.
- TPE ì•Œê³ ë¦¬ì¦˜ì€ ì‹¤ì œë¡œ ì´í›„ ë‹¨ê³„ì—ì„œë„ ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì§€ì†ì ìœ¼ë¡œ ê°œì„ í•˜ëŠ” ë°˜ë©´, Random searchëŠ” ì²˜ìŒì— ìƒë‹¹íˆ ì¢‹ì€ ì†”ë£¨ì…˜ì„ ë¬´ì‘ìœ„ë¡œ ì°¾ì€ ë‹¤ìŒ ê²°ê³¼ë¥¼ ì•½ê°„ë§Œ ê°œì„ í–ˆìŠµë‹ˆë‹¤. 
- TPEì™€ RandomizedSearch ê²°ê³¼ì˜ í˜„ì¬ ì°¨ì´ëŠ” ë§¤ìš° ì‘ì§€ë§Œ, ë” ë‹¤ì–‘í•œ ë²”ìœ„ì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ì¼ë¶€ ì‹¤ì œ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ hyperoptëŠ” ìƒë‹¹í•œ ì‹œê°„ ëŒ€ë¹„ ì ìˆ˜ í–¥ìƒì„ ì œê³µ í•  ìˆ˜ ìˆìœ¼ë¦¬ë¼ ë´…ë‹ˆë‹¤. 

- ì°¸ê³  : ì‹¤ì œ ìƒí™œì—ì„œëŠ” ë¹„êµë¥¼ ìœ„í•´ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•˜ì§€ ì•Šê³  ì‹œê°„ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì •í™•í•˜ì§€ë§Œ ì¥ë‚œê° ì˜ˆì œì—ì„œëŠ” tpe ë° ì–´ë‹ë§ì˜ ì¶”ê°€ ê³„ì‚°ì— ì†Œìš”ë˜ëŠ” ì‹œê°„ì˜ ë¹„ìœ¨ì´ cross_val_score ê³„ì‚° ì‹œê°„ì— ë¹„í•´ ë†’ìœ¼ë¯€ë¡œ ë°˜ë³µ íšŸìˆ˜ì™€ ê´€ë ¨í•˜ì—¬ í•˜ì´í¼ ì˜µíŠ¸ ë° í”Œë¡¯ ì ìˆ˜ì˜ ê³„ì‚° ì†ë„ì— ëŒ€í•´ ì˜¤í•´í•˜ì§€ ì•Šê¸°ë¡œ ê²°ì •í–ˆìŠµë‹ˆë‹¤.

### ì‹¤ì œ Evaluate í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì´ìš©í•´ ê²°ê³¼ë¥¼  ë¹„êµí•˜ê³  êµì°¨ ê²€ì¦ ê²°ê³¼ì™€ inline í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤.


```python
print('Test MSE scored:')
print("Grid Search : {:.3f}".format(gs_test_score))
print("Random Search :  {:.3f}".format(rs_test_score))
print("Hyperband : {:.3f}".format(hb_test_score))
print("Bayesian optimization : {:.3f}".format(bayes_test_score))
print("TPE : {:.3f}".format(tpe_test_score))


```

    Test MSE scored:
    Grid Search : 3045.329
    Random Search :  2877.117
    Hyperband : 2852.900
    Bayesian optimization : 2710.621
    TPE : 2942.574


Test dataì˜ evaluationì—ì„œëŠ” Bayesian optimization ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ëª¨ë¸ MSE ì ìˆ˜ê°€ ê°€ì¥ ë‚®ì€ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. (ì‹¤í—˜ìš© Toy datasetìœ¼ë¡œ ì‹¤í–‰ì— ë”°ë¼ ê²°ê³¼ì„ì„ ì°¸ê³ )

- Accuinsight+ì˜ modeler AutoDLì— ì ìš©í•œ ë‹¤ì–‘í•œ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ìµœì í™” ì ‘ê·¼ ë°©ì‹ì— ëŒ€í•´ ì•Œì•„ë´¤ìŠµë‹ˆë‹¤.
- ìµœì‹  hyperoptì˜ TPEì•Œê³ ë¦¬ì¦˜ì˜ ì‚¬ìš©ë°©ë²•ì„ ì•Œ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. 
- ì‹¤ì œ ëª¨ë¸ë§ í™˜ê²½ì—ì„œëŠ”, ì‹¤ì œë¡œ ì–´ë–¤ ì ‘ê·¼ ë°©ì‹ì´ ê°€ì¥ ì¢‹ì€ì§€ ë¯¸ë¦¬ ì•Œ ìˆ˜ ì—†ìœ¼ë©°, ë•Œë¡œëŠ” ê°„ë‹¨í•œ RandomizedSearchê°€ ì¢‹ì€ ì„ íƒì´ ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ í•­ìƒ ì•Œì•„ë‘ë©´ ìœ ìš©í•©ë‹ˆë‹¤. 
- ì´ íŠœí† ë¦¬ì–¼ì´ í–¥í›„ ML, DL í”„ë¡œì íŠ¸ì—ì„œ ë§ì€ ì‹œê°„ì„ ì ˆì•½í•˜ê¸°ë¥¼ ë°”ëë‹ˆë‹¤.
